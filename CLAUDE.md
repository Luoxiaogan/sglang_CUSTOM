# SGLang Testing Framework - ç”¨æˆ·ä½¿ç”¨æ‰‹å†Œ

## å¿«é€Ÿå¼€å§‹ï¼ˆ5åˆ†é’Ÿä¸Šæ‰‹ï¼‰

### 1. å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/sgl-project/sglang.git
cd sglang/sglang_test_framework

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# ç¡®ä¿ SGLang å·²å®‰è£…
pip install sglang

# å¯¹äºè·¯ç”±æµ‹è¯•ï¼Œè¿˜éœ€è¦å®‰è£… sglang-router
# æ–¹æ³• 1: ä» PyPI å®‰è£…
pip install sglang-router

# æ–¹æ³• 2: ä»æºç å®‰è£…ï¼ˆæ¨èç”¨äºå¼€å‘ï¼‰
cd ../sgl-router
pip install -e .
cd ../sglang_test_framework
```

### 2. è¿è¡Œç¬¬ä¸€ä¸ªæµ‹è¯•

```python
from sglang_test_framework import NodeTest, NodeConfig

# é…ç½®æµ‹è¯•
config = NodeConfig(
    model_path="meta-llama/Llama-2-7b-hf",
    gpu_id=0,
    request_rate=10.0,  # æ¯ç§’10ä¸ªè¯·æ±‚
    num_prompts=100     # æ€»å…±100ä¸ªè¯·æ±‚
)

# è¿è¡Œæµ‹è¯•
test = NodeTest(config)
results = test.run()

# æŸ¥çœ‹ç»“æœ
test.analyze_results(results)
```

å°±è¿™ä¹ˆç®€å•ï¼æµ‹è¯•ä¼šè‡ªåŠ¨å¯åŠ¨æœåŠ¡å™¨ã€å‘é€è¯·æ±‚ã€æ”¶é›†æŒ‡æ ‡å¹¶ç”ŸæˆæŠ¥å‘Šã€‚

## æ ¸å¿ƒåŠŸèƒ½ä»‹ç»

### æµ‹è¯•æ¨¡å¼

#### å•èŠ‚ç‚¹æµ‹è¯• - æµ‹è¯•å•ä¸ª GPU çš„æ€§èƒ½æé™
```python
NodeConfig(
    model_path="meta-llama/Llama-2-7b-hf",
    gpu_id=0,
    max_running_requests=256,  # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    tp_size=1,                 # å¼ é‡å¹¶è¡Œå¤§å°
    mem_fraction_static=0.9    # é™æ€å†…å­˜åˆ†é…æ¯”ä¾‹
)
```

#### å¤šèŠ‚ç‚¹è·¯ç”±æµ‹è¯• - æµ‹è¯•å¤š GPU è´Ÿè½½å‡è¡¡
```python
# æ³¨æ„ï¼šè·¯ç”±æµ‹è¯•éœ€è¦å…ˆå®‰è£… sglang-routerï¼ˆè§å®‰è£…è¯´æ˜ï¼‰
RoutingConfig(
    model_path="meta-llama/Llama-2-7b-hf",
    num_gpus=4,
    routing_policy="cache_aware",  # è·¯ç”±ç­–ç•¥
    request_rate=40.0
)
```

### å…³é”®æŒ‡æ ‡è¯´æ˜

- **Server Latency**: è¯·æ±‚åœ¨æœåŠ¡å™¨ä¸Šçš„å¤„ç†æ—¶é—´
- **Total Latency**: ä»è¯·æ±‚åˆ°è¾¾åˆ°å®Œæˆçš„æ€»æ—¶é—´ï¼ˆåŒ…æ‹¬æ’é˜Ÿï¼‰
- **TTFT**: ç”Ÿæˆç¬¬ä¸€ä¸ª token çš„æ—¶é—´
- **Queue Time**: è¯·æ±‚åœ¨é˜Ÿåˆ—ä¸­çš„ç­‰å¾…æ—¶é—´

## å¸¸è§æµ‹è¯•åœºæ™¯

### åœºæ™¯ 1: å¯»æ‰¾æœ€ä½³å¹¶å‘æ•°

```python
# æµ‹è¯•ä¸åŒçš„ max_running_requests å€¼
for mrs in [32, 64, 128, 256, 512]:
    config = NodeConfig(
        model_path="meta-llama/Llama-2-7b-hf",
        max_running_requests=mrs,
        request_rate=20.0,
        num_prompts=1000
    )
    
    test = NodeTest(config)
    results = test.run()
    
    print(f"MRS={mrs}: "
          f"Throughput={results['metrics']['request_throughput']:.2f} req/s, "
          f"P99 Latency={results['metrics']['p99_server_latency']:.1f} ms")
```

### åœºæ™¯ 2: å‹åŠ›æµ‹è¯•

```python
# é€æ­¥å¢åŠ è´Ÿè½½ï¼Œæ‰¾åˆ°ç³»ç»Ÿæé™
config = NodeConfig(
    model_path="meta-llama/Llama-2-7b-hf",
    request_rate=float('inf'),  # æœ€å¤§é€Ÿç‡å‘é€
    num_prompts=5000
)

test = NodeTest(config)
results = test.run()

# æŸ¥çœ‹ç³»ç»Ÿåœ¨æé™è´Ÿè½½ä¸‹çš„è¡¨ç°
print(f"æœ€å¤§ååé‡: {results['metrics']['request_throughput']:.2f} req/s")
print(f"é”™è¯¯ç‡: {(1 - results['success_rate']) * 100:.1f}%")
```

### åœºæ™¯ 3: è·¯ç”±ç­–ç•¥å¯¹æ¯”

```python
# æµ‹è¯•ä¸åŒè·¯ç”±ç­–ç•¥çš„æ•ˆæœ
for policy in ["cache_aware", "round_robin", "shortest_queue"]:
    config = RoutingConfig(
        model_path="meta-llama/Llama-2-7b-hf",
        num_gpus=4,
        routing_policy=policy,
        request_rate=50.0,
        num_prompts=2000
    )
    
    test = RoutingTest(config)
    results = test.run()
    test.visualize_results(results)
```

### åœºæ™¯ 4: é•¿çŸ­è¯·æ±‚æ··åˆæµ‹è¯•

```python
# ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†æµ‹è¯•ä¸åŒé•¿åº¦è¯·æ±‚çš„å¤„ç†
config = NodeConfig(
    model_path="meta-llama/Llama-2-7b-hf",
    dataset_name="random",
    random_input_len=1024,      # å¹³å‡è¾“å…¥é•¿åº¦
    random_output_len=256,      # å¹³å‡è¾“å‡ºé•¿åº¦
    random_range_ratio=0.8,     # é•¿åº¦å˜åŒ–èŒƒå›´ Â±80%
    request_rate=15.0
)
```

## é…ç½®å‚æ•°è¯¦è§£

### NodeConfig - å•èŠ‚ç‚¹æµ‹è¯•é…ç½®

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|-----|--------|------|
| model_path | å¿…éœ€ | æ¨¡å‹è·¯å¾„ |
| gpu_id | 0 | ä½¿ç”¨çš„ GPU ID |
| max_running_requests | 256 | æœ€å¤§å¹¶å‘è¯·æ±‚æ•° |
| tp_size | 1 | å¼ é‡å¹¶è¡Œå¤§å° |
| mem_fraction_static | 0.9 | é™æ€å†…å­˜åˆ†é…æ¯”ä¾‹ |
| request_rate | inf | è¯·æ±‚å‘é€é€Ÿç‡ï¼ˆreq/sï¼‰ |
| num_prompts | 1000 | æµ‹è¯•è¯·æ±‚æ€»æ•° |
| dataset_name | "sharegpt" | æ•°æ®é›†ç±»å‹ |

### RoutingConfig - è·¯ç”±æµ‹è¯•é…ç½®

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|-----|--------|------|
| num_gpus | 4 | GPU æ•°é‡ |
| routing_policy | "cache_aware" | è·¯ç”±ç­–ç•¥ |
| base_port | 30001 | èµ·å§‹ç«¯å£å· |
| collect_per_node_metrics | True | æ˜¯å¦æ”¶é›†æ¯ä¸ªèŠ‚ç‚¹çš„æŒ‡æ ‡ |

### è·¯ç”±ç­–ç•¥è¯´æ˜

- **cache_aware**: ä¼˜å…ˆè·¯ç”±åˆ°æœ‰ç¼“å­˜çš„èŠ‚ç‚¹ï¼ˆSGLang Router é»˜è®¤ï¼‰
- **round_robin**: è½®è¯¢åˆ†é…
- **random**: éšæœºåˆ†é…
- **shortest_queue**: é€‰æ‹©é˜Ÿåˆ—æœ€çŸ­çš„èŠ‚ç‚¹

## ç»“æœåˆ†ææŒ‡å—

### æŸ¥çœ‹æµ‹è¯•æŠ¥å‘Š

æµ‹è¯•å®Œæˆåä¼šè‡ªåŠ¨ç”Ÿæˆï¼š

1. **æ§åˆ¶å°æ‘˜è¦**ï¼šå…³é”®æŒ‡æ ‡çš„æ–‡æœ¬æŠ¥å‘Š
2. **CSV æ–‡ä»¶**ï¼šæ¯ä¸ªè¯·æ±‚çš„è¯¦ç»†æ•°æ®
3. **JSON æ–‡ä»¶**ï¼šå®Œæ•´çš„æµ‹è¯•ç»“æœå’Œé…ç½®
4. **å¯è§†åŒ–å›¾è¡¨**ï¼šæ€§èƒ½åˆ†æå›¾è¡¨ï¼ˆè·¯ç”±æµ‹è¯•ï¼‰

### å…³é”®æŒ‡æ ‡è§£è¯»

```
ğŸ“Š Performance Metrics:
  Success Rate: 99.8%              # æˆåŠŸç‡åº” > 99%
  Request Throughput: 45.2 req/s   # å®é™…å¤„ç†èƒ½åŠ›
  Token Throughput:
    Input: 23,040 tok/s            # è¾“å…¥ token å¤„ç†é€Ÿåº¦
    Output: 5,760 tok/s            # è¾“å‡º token ç”Ÿæˆé€Ÿåº¦

â± Latency Metrics:
  Server Latency (ms):
    Mean: 245.1                    # å¹³å‡å¤„ç†æ—¶é—´
    P95: 489.2                     # 95% è¯·æ±‚çš„å»¶è¿Ÿä¸Šé™
    P99: 612.8                     # 99% è¯·æ±‚çš„å»¶è¿Ÿä¸Šé™

  Queue Time (ms):
    Mean: 67.4                     # å¹³å‡æ’é˜Ÿæ—¶é—´
    P95: 145.2                     # é˜Ÿåˆ—å»¶è¿Ÿåº”è¯¥è¾ƒä½
```

### æ€§èƒ½ç“¶é¢ˆè¯†åˆ«

1. **é«˜é˜Ÿåˆ—æ—¶é—´**ï¼šå¢åŠ  `max_running_requests`
2. **é«˜æœåŠ¡å™¨å»¶è¿Ÿ**ï¼šå‡å°‘ `max_running_requests` æˆ–ä½¿ç”¨æ›´å¤§çš„ GPU
3. **å†…å­˜ä¸è¶³**ï¼šé™ä½ `mem_fraction_static` æˆ–å‡å°‘å¹¶å‘æ•°
4. **è´Ÿè½½ä¸å‡**ï¼šæ£€æŸ¥è·¯ç”±ç­–ç•¥ï¼Œè€ƒè™‘ä½¿ç”¨ `shortest_queue`

## é«˜çº§åŠŸèƒ½

### ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†

```python
# æ–¹æ³• 1: ä½¿ç”¨ ShareGPT æ•°æ®é›†
config = NodeConfig(
    model_path="meta-llama/Llama-2-7b-hf",
    dataset_name="sharegpt",
    dataset_path="/path/to/sharegpt.json"  # å¯é€‰ï¼Œä¼šè‡ªåŠ¨ä¸‹è½½
)

# æ–¹æ³• 2: ä½¿ç”¨éšæœºç”Ÿæˆçš„æ•°æ®
config = NodeConfig(
    model_path="meta-llama/Llama-2-7b-hf",
    dataset_name="random",
    random_input_len=512,
    random_output_len=128
)

# æ–¹æ³• 3: ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†
# åˆ›å»ºç¬¦åˆæ ¼å¼çš„ JSON æ–‡ä»¶ï¼š
# [{"prompt": "...", "completion": "...", "id": "..."}, ...]
```

### å¯¼å‡ºå’Œåˆ†ææ•°æ®

```python
# æµ‹è¯•ç»“æœä¼šè‡ªåŠ¨å¯¼å‡ºï¼Œä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤„ç†
results = test.run()

# è·å– CSV è·¯å¾„è¿›è¡Œè‡ªå®šä¹‰åˆ†æ
csv_path = results['exported_files']['csv']

# ä½¿ç”¨ pandas åˆ†æ
import pandas as pd
df = pd.read_csv(csv_path)

# è®¡ç®—è‡ªå®šä¹‰æŒ‡æ ‡
print(f"ä¸­ä½æ•°å»¶è¿Ÿ: {df['server_latency'].median() * 1000:.1f} ms")
print(f"ååé‡æ ‡å‡†å·®: {df.groupby(pd.cut(df['arrival_time'], bins=10))['req_id'].count().std():.2f}")
```

### èŠ‚ç‚¹æ•…éšœæ¨¡æ‹Ÿï¼ˆè·¯ç”±æµ‹è¯•ï¼‰

```python
config = RoutingConfig(
    model_path="meta-llama/Llama-2-7b-hf",
    num_gpus=4,
    enable_node_failures=True,
    failure_schedule={
        300: [1, 2],  # 300ç§’æ—¶èŠ‚ç‚¹ 1ã€2 æ•…éšœ
        600: []       # 600ç§’æ—¶æ¢å¤æ‰€æœ‰èŠ‚ç‚¹
    }
)
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### é«˜ååé‡é…ç½®

```yaml
é€‚ç”¨åœºæ™¯: æ‰¹é‡æ¨ç†ã€ç¦»çº¿å¤„ç†
é…ç½®å»ºè®®:
  max_running_requests: 512
  mem_fraction_static: 0.85
  chunked_prefill_size: 8192
  enable_torch_compile: true
é¢„æœŸæ•ˆæœ: ååé‡æå‡ 50-100%ï¼Œå»¶è¿Ÿå¢åŠ  20-30%
```

### ä½å»¶è¿Ÿé…ç½®

```yaml
é€‚ç”¨åœºæ™¯: å®æ—¶æœåŠ¡ã€äº¤äº’å¼åº”ç”¨
é…ç½®å»ºè®®:
  max_running_requests: 32
  mem_fraction_static: 0.9
  schedule_conservativeness: 1.2
é¢„æœŸæ•ˆæœ: P99 å»¶è¿Ÿé™ä½ 30-50%ï¼Œååé‡é™ä½ 20-30%
```

### å†…å­˜ä¼˜åŒ–é…ç½®

```yaml
é€‚ç”¨åœºæ™¯: GPU å†…å­˜å—é™
é…ç½®å»ºè®®:
  max_running_requests: 128
  mem_fraction_static: 0.7
  quantization: "fp8"
é¢„æœŸæ•ˆæœ: å†…å­˜ä½¿ç”¨é™ä½ 30-40%ï¼Œè½»å¾®æ€§èƒ½æŸå¤±
```

## æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

#### 1. æœåŠ¡å™¨å¯åŠ¨å¤±è´¥
```
é”™è¯¯: "Address already in use"
è§£å†³: ä¿®æ”¹ port å‚æ•°æˆ–æ£€æŸ¥ç«¯å£å ç”¨
```

#### 2. GPU å†…å­˜ä¸è¶³
```
é”™è¯¯: "CUDA out of memory"
è§£å†³: 
- å‡å°‘ max_running_requests
- é™ä½ mem_fraction_static
- ä½¿ç”¨é‡åŒ–ï¼ˆquantization="fp8"ï¼‰
```

#### 3. è¯·æ±‚è¶…æ—¶
```
é”™è¯¯: "Request timeout"
è§£å†³:
- æ£€æŸ¥æ¨¡å‹åŠ è½½æ˜¯å¦æˆåŠŸ
- å¢åŠ  warmup_requests æ•°é‡
- ç¡®è®¤ GPU æ²¡æœ‰è¢«å…¶ä»–è¿›ç¨‹å ç”¨
```

#### 4. ç»“æœä¸ä¸€è‡´
```
é—®é¢˜: å¤šæ¬¡æµ‹è¯•ç»“æœå·®å¼‚å¾ˆå¤§
è§£å†³:
- å¢åŠ  num_promptsï¼ˆå»ºè®® > 1000ï¼‰
- ä½¿ç”¨å›ºå®šçš„ seed
- ç¡®ä¿ç³»ç»Ÿè´Ÿè½½ç¨³å®š
```

## æœ€ä½³å®è·µ

### æµ‹è¯•å‰å‡†å¤‡
1. ç¡®ä¿ GPU ç©ºé—²ï¼š`nvidia-smi`
2. å…³é—­å…¶ä»– GPU è¿›ç¨‹
3. é¢„ç•™è¶³å¤Ÿçš„æµ‹è¯•æ—¶é—´ï¼ˆå»ºè®®æ¯æ¬¡æµ‹è¯• > 5 åˆ†é’Ÿï¼‰

### æµ‹è¯•æ‰§è¡Œ
1. å…ˆç”¨å°è§„æ¨¡æµ‹è¯•éªŒè¯é…ç½®
2. é€æ­¥å¢åŠ è´Ÿè½½æ‰¾åˆ°æœ€ä½³å·¥ä½œç‚¹
3. é‡å¤æµ‹è¯• 3 æ¬¡ä»¥ä¸Šç¡®ä¿ç»“æœç¨³å®š

### ç»“æœéªŒè¯
1. æ£€æŸ¥æˆåŠŸç‡æ˜¯å¦ > 99%
2. è§‚å¯Ÿå»¶è¿Ÿåˆ†å¸ƒæ˜¯å¦æœ‰å¼‚å¸¸å³°å€¼
3. ç¡®è®¤ååé‡æ˜¯å¦éšæ—¶é—´ç¨³å®š

## API å¿«é€Ÿå‚è€ƒ

### åŸºç¡€ç”¨æ³•
```python
from sglang_test_framework import NodeTest, NodeConfig

config = NodeConfig(model_path="...", gpu_id=0)
test = NodeTest(config)
results = test.run()
```

### è·¯ç”±æµ‹è¯•
```python
from sglang_test_framework import RoutingTest, RoutingConfig

config = RoutingConfig(model_path="...", num_gpus=4)
test = RoutingTest(config)
results = test.run()
test.visualize_results(results)
```

### ç»“æœåˆ†æ
```python
# è‡ªåŠ¨ç”Ÿæˆçš„æ–‡ä»¶
results['exported_files']['csv']      # è¯¦ç»†æ•°æ®
results['exported_files']['json']     # å®Œæ•´ç»“æœ
results['metrics']                    # èšåˆæŒ‡æ ‡
```

## æ›´å¤šèµ„æº

- **æŠ€æœ¯è®¾è®¡æ–‡æ¡£**ï¼š`sglang_test_framework/è¯´æ˜.md`
- **SGLang å®˜æ–¹æ–‡æ¡£**ï¼š`.DUCUMENT/` ç›®å½•
- **é—®é¢˜åé¦ˆ**ï¼šhttps://github.com/sgl-project/sglang/issues

## ç‰ˆæœ¬å…¼å®¹æ€§

- SGLang >= 0.3.0
- SGLang Router >= 0.1.0
- Python >= 3.8
- CUDA >= 11.0

## SGLang Router æµ‹è¯•æŒ‡å—

### 1. å¯åŠ¨ SGLang æœåŠ¡å™¨

```bash
# ç»ˆç«¯ 1 - GPU 2
python -m sglang.launch_server \
--model-path "/nas/models/Meta-Llama-3-8B-Instruct" \
--host "0.0.0.0" \
--port 60005 \
--base-gpu-id 2 \
--enable-metrics \  # æ·»åŠ è¿™ä¸ªå‚æ•°,collect_metrics åªåœ¨ self.enable_metrics ä¸º True æ—¶è°ƒç”¨
--log-level debug # è¿™ä¸ªå¯ä»¥åœ¨serverçš„logé‡Œé¢æ£€æŸ¥ç›¸åº”çš„å…¨éƒ¨çš„log

# ç»ˆç«¯ 2 - GPU 3
python -m sglang.launch_server \
--model-path "/nas/models/Meta-Llama-3-8B-Instruct" \
--host "0.0.0.0" \
--port 60006 \
--base-gpu-id 3 \
--enable-metrics
```

### 2. å¯åŠ¨è·¯ç”±å™¨ï¼ˆå¯ç”¨è¯·æ±‚è¿½è¸ªï¼‰

```bash
bash /nas/ganluo/sglang/bash_start_router.sh
```
åœæ­¢çš„æ–¹æ³•æ˜¯:
```bash
ctrl+Z
kill %1
clear
```

### 3. æµ‹è¯•router
```bash
bash /nas/ganluo/sglang/bash_send_req.sh
```

### 4. æ•…éšœæ’é™¤
å½“é‡åˆ°29000ç«¯å£è¢«å æ®çš„æƒ…å†µ, æ˜¯å› ä¸ºä¹‹å‰çš„routerç«¯å£æ²¡æœ‰é‡Šæ”¾, éœ€è¦è¿™æ ·åš
```bash
lsof -i :29000
```
ç„¶å`kill -9`è¿™ä¸ªpid.

### 5. Routeræµ‹è¯•å‚æ•°è§£è¯»
ç›®å‰è®°å½•åœ¨csvä¸­çš„å‚æ•°æœ‰ï¼ˆæŒ‰ç…§æ–°çš„é¡ºåºï¼‰:

**åŸºç¡€ä¿¡æ¯ï¼š**
req_id,success,error,host,has_generated_text

**é•¿åº¦ç›¸å…³å­—æ®µï¼š**
input_length,expected_output_length,actual_prompt_tokens,actual_output_tokens,actual_total_tokens,output_tokens_from_trace,decode_length

**æ—¶é—´æˆ³ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰ï¼š**
arrival_time,router_send_time,server_created_time,queue_time_start,queue_time_end,server_first_token_time,finish_time

**å»¶è¿ŸæŒ‡æ ‡ï¼š**
server_latency,total_latency,ttft,queue_time_in_router,queue_time_in_server,tokenize_time,pure_queue_time

#### å‚æ•°å«ä¹‰è¯¦è§£ï¼š

**åŸºç¡€ä¿¡æ¯ï¼š**
- `req_id`: è¯·æ±‚çš„å”¯ä¸€æ ‡è¯†ç¬¦
- `success`: è¯·æ±‚æ˜¯å¦æˆåŠŸï¼ˆTrue/Falseï¼‰
- `error`: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
- `host`: å®é™…å¤„ç†è¯·æ±‚çš„æœåŠ¡å™¨åœ°å€
- `has_generated_text`: æ˜¯å¦æœ‰ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆç”¨äºè°ƒè¯•ï¼‰

**é•¿åº¦ç›¸å…³å­—æ®µè¯´æ˜ï¼ˆé‡è¦ï¼‰ï¼š**
- `input_length`: è¾“å…¥æ–‡æœ¬çš„**å•è¯æ•°**ï¼ˆç”± `prompt.split()` è®¡ç®—ï¼Œä»¥ç©ºæ ¼åˆ†å‰²ï¼‰
- `expected_output_length`: è¯·æ±‚æ—¶è®¾ç½®çš„æœŸæœ›è¾“å‡º **token æ•°**ï¼ˆmax_new_tokens å‚æ•°ï¼‰
- `actual_prompt_tokens`: è¾“å…¥æ–‡æœ¬ç»è¿‡ tokenizer å¤„ç†åçš„å®é™… **token æ•°**ï¼ˆä»æœåŠ¡å™¨è¿”å›ï¼‰
- `actual_output_tokens`: å®é™…ç”Ÿæˆçš„ **token æ•°**ï¼ˆä»æœåŠ¡å™¨è¿”å›ï¼‰
- `actual_total_tokens`: å®é™…æ€» **token æ•°**ï¼ˆè¾“å…¥+è¾“å‡ºï¼Œä»æœåŠ¡å™¨è¿”å›ï¼‰
- `output_tokens_from_trace`: ä»è·¯ç”±å™¨è¿½è¸ªä¿¡æ¯è·å–çš„è¾“å‡º **token æ•°**ï¼ˆé€šå¸¸ä¸º 0ï¼‰
- `decode_length`: ç»¼åˆå­—æ®µï¼Œè¡¨ç¤ºç”Ÿæˆçš„ **token æ•°**ï¼ˆä¼˜å…ˆä½¿ç”¨ actual_output_tokensï¼‰

**æ³¨æ„äº‹é¡¹ï¼š**
1. **å•è¯æ•° vs Token æ•°**ï¼š
   - `input_length` æ˜¯æŒ‰ç©ºæ ¼åˆ†å‰²çš„å•è¯æ•°ï¼Œä¸æ˜¯ token æ•°
   - å¯¹äºè‹±æ–‡ï¼šä¸€ä¸ªå•è¯å¯èƒ½å¯¹åº” 1-3 ä¸ª tokens
   - å¯¹äºä¸­æ–‡ï¼šç”±äºä¸­æ–‡é€šå¸¸ä¸ä½¿ç”¨ç©ºæ ¼åˆ†å‰²ï¼Œ`input_length` å¯èƒ½ä¸å‡†ç¡®
   - å› æ­¤ `actual_prompt_tokens` æ‰æ˜¯çœŸå®çš„è¾“å…¥ token æ•°é‡

2. **å‡†ç¡®çš„ Token è®¡æ•°**ï¼š
   - **å¿…é¡»ä½¿ç”¨** `actual_prompt_tokens` è€Œä¸æ˜¯ `input_length` æ¥è®¡ç®—è¾“å…¥ token ååé‡
   - **å¿…é¡»ä½¿ç”¨** `actual_output_tokens` è€Œä¸æ˜¯ `decode_length` æ¥è®¡ç®—è¾“å‡º token ååé‡
   
3. **ç¤ºä¾‹å¯¹æ¯”**ï¼š
   - è‹±æ–‡æ–‡æœ¬ "Hello world": `input_length`=2ï¼ˆ2ä¸ªå•è¯ï¼‰ï¼Œ`actual_prompt_tokens` å¯èƒ½æ˜¯ 2-3
   - ä¸­æ–‡æ–‡æœ¬ "ä½ å¥½ä¸–ç•Œ": `input_length`=1ï¼ˆæ²¡æœ‰ç©ºæ ¼ï¼‰ï¼Œ`actual_prompt_tokens` å¯èƒ½æ˜¯ 4-6

**æ—¶é—´æˆ³ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰ï¼š**
- `arrival_time`: è¯·æ±‚åˆ°è¾¾routerçš„æ—¶åˆ»ï¼ˆä»æµ‹è¯•å¼€å§‹è®¡æ—¶ï¼‰
- `router_send_time`: routerå°†è¯·æ±‚è½¬å‘åˆ°serverçš„æ—¶åˆ»ï¼ˆåŸå to_server_timeï¼‰
- `server_created_time`: è¯·æ±‚åœ¨serverç«¯è¢«tokenizer_manageræ¥æ”¶çš„æ—¶åˆ»
  - æ ¹æ®`python/sglang/srt/entrypoints/http_server.py`, server_created_time æ˜¯è¯·æ±‚è¢« tokenizer_manager æ¥æ”¶çš„æ—¶åˆ»ï¼Œè€Œä¸æ˜¯ HTTP è¯·æ±‚åˆšåˆ°è¾¾æœåŠ¡å™¨çš„æ—¶åˆ»
  - ä½†æ˜¯è¯·æ±‚åˆ°è¾¾æœåŠ¡å™¨ååŸºæœ¬ä¸Šæ˜¯ç«‹å³è½¬å‘åˆ° tokenizer_manager çš„ï¼Œä¸­é—´æ²¡æœ‰æ’é˜Ÿ
- `queue_time_start`: è¯·æ±‚è¿›å…¥scheduler waiting_queueçš„æ—¶åˆ»
- `queue_time_end`: è¯·æ±‚ä»waiting_queueå–å‡ºå‡†å¤‡å¤„ç†çš„æ—¶åˆ»
- `server_first_token_time`: serverç”Ÿæˆç¬¬ä¸€ä¸ªtokençš„æ—¶åˆ»ï¼ˆprefillå®Œæˆ+ç¬¬ä¸€æ¬¡decodeï¼‰
- `finish_time`: è¯·æ±‚å®Œæˆçš„æ—¶åˆ»

**å»¶è¿ŸæŒ‡æ ‡ï¼š**
- `server_latency`: æœåŠ¡å™¨ä¸­çš„æ€»æ—¶é—´ = `finish_time - router_send_time`
- `total_latency`: æ€»å»¶è¿Ÿ = `finish_time - arrival_time`
- `ttft` (Time To First Token): ç¬¬ä¸€ä¸ªtokenç”Ÿæˆæ—¶é—´ = `server_first_token_time - arrival_time`ï¼ˆä»å®¢æˆ·ç«¯å‘é€æ—¶åˆ»ç®—èµ·ï¼‰
- `queue_time_in_router`: routerä¸­çš„æ’é˜Ÿæ—¶é—´ = `router_send_time - arrival_time`ï¼ˆé€šå¸¸å¾ˆå°ï¼‰
- `queue_time_in_server`: serverç«¯æ€»æ’é˜Ÿæ—¶é—´ = `server_first_token_time - server_created_time`
- `tokenize_time`: serverç«¯tokenizeçš„æ—¶é—´ = `queue_time_start - server_created_time`
- `pure_queue_time`: schedulerçº¯æ’é˜Ÿæ—¶é—´ = `queue_time_end - queue_time_start`ï¼ˆä¸å«tokenizeæ—¶é—´ï¼‰

**å„é˜¶æ®µè€—æ—¶è®¡ç®—ï¼š**
1. **Routerè½¬å‘å»¶è¿Ÿ**: `router_send_time - arrival_time` ï¼ˆé€šå¸¸<0.1msï¼‰
2. **Tokenizeæ—¶é—´**: `queue_time_start - server_created_time`
3. **Scheduleræ’é˜Ÿæ—¶é—´**: `queue_time_end - queue_time_start`ï¼ˆå³ pure_queue_timeï¼‰
4. **Prefillæ—¶é—´**: `server_first_token_time - queue_time_end`
5. **ç½‘ç»œä¼ è¾“æ—¶é—´**: `server_created_time - router_send_time`

### 6. å…³äº decode_length å’Œ token è®¡æ•°è¯´æ˜ï¼ˆå·²ä¿®å¤ï¼‰

#### é—®é¢˜èƒŒæ™¯
ä¹‹å‰æµ‹è¯•ä¸­ `decode_length` æ˜¾ç¤ºä¸º 0 æ˜¯å› ä¸ºä¾èµ–è·¯ç”±å™¨è¿½è¸ªä¿¡æ¯ï¼Œä½†è¯¥ä¿¡æ¯å¯èƒ½ä¸å®Œæ•´ã€‚

#### ä¿®å¤æ–¹æ¡ˆï¼ˆå·²å®æ–½ï¼‰
ç°åœ¨æµ‹è¯•ä»£ç å·²ç»æ›´æ–°ï¼Œé‡‡ç”¨ä¸ SGLang å®˜æ–¹ bench_serving.py ç›¸åŒçš„æ–¹æ³•è·å–å®é™…ç”Ÿæˆçš„ token æ•°ï¼š

1. **ä» meta_info æå–**ï¼ˆSGLang native APIï¼‰ï¼š
   - `completion_tokens`: å®é™…ç”Ÿæˆçš„ token æ•°
   - `prompt_tokens`: å®é™…è¾“å…¥çš„ token æ•°
   - `total_tokens`: æ€» token æ•°

2. **ä» usage å­—æ®µæå–**ï¼ˆOpenAI-compatible APIï¼‰ï¼š
   - ä½œä¸º meta_info çš„å¤‡é€‰æ–¹æ¡ˆ

3. **CSV ä¸­è®°å½•çš„æ‰€æœ‰ token ç›¸å…³å­—æ®µ**ï¼š
   - `expected_output_length`: è¯·æ±‚æ—¶è®¾ç½®çš„æœŸæœ›è¾“å‡ºé•¿åº¦
   - `actual_output_tokens`: ä» meta_info/usage è·å–çš„å®é™…ç”Ÿæˆ token æ•°
   - `actual_prompt_tokens`: ä» meta_info/usage è·å–çš„å®é™…è¾“å…¥ token æ•°
   - `actual_total_tokens`: ä» meta_info/usage è·å–çš„æ€» token æ•°
   - `output_tokens_from_trace`: ä»è·¯ç”±å™¨è¿½è¸ªä¿¡æ¯è·å–çš„è¾“å‡º token æ•°
   - `decode_length`: ç»¼åˆå­—æ®µï¼ˆä¼˜å…ˆä½¿ç”¨ actual_output_tokensï¼Œä¿æŒå‘åå…¼å®¹ï¼‰
   - `has_generated_text`: æ˜¯å¦æœ‰ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆç”¨äºè°ƒè¯•ï¼‰

#### ä½¿ç”¨è¯´æ˜
- åˆ†ææ—¶ä¼˜å…ˆä½¿ç”¨ `actual_output_tokens` å­—æ®µï¼Œè¿™æ˜¯æœ€å‡†ç¡®çš„å®é™…ç”Ÿæˆé•¿åº¦
- `decode_length` ä¿ç•™ç”¨äºå‘åå…¼å®¹
- å¦‚æœ `actual_output_tokens` ä¸ºç©ºï¼Œå¯èƒ½æ˜¯æœåŠ¡å™¨ç‰ˆæœ¬è¾ƒæ—§æˆ–æœªå¯ç”¨ç›¸å…³åŠŸèƒ½

---

*æœ¬æ¡†æ¶æŒç»­æ›´æ–°ä¸­ï¼Œæ¬¢è¿è´¡çŒ®ä»£ç å’Œåé¦ˆé—®é¢˜ï¼*