#!/usr/bin/env python3
"""
é€šè¿‡è‡ªå®šä¹‰é‡‡æ ·å‚æ•°è¿½è¸ªGPUåˆ†é…ã€‚
æ¯ä¸ªGPUä½¿ç”¨ç•¥å¾®ä¸åŒçš„temperatureï¼Œé€šè¿‡è¾“å‡ºçš„å¤šæ ·æ€§æ¨æ–­ã€‚
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import asyncio
import json
from typing import Dict, List
import aiohttp
import numpy as np
from collections import defaultdict

from sglang_test_framework import RoutingConfig
from sglang_test_framework.core import ServerManager, RequestGenerator

class GPUTrackingTest:
    """ä½¿ç”¨ä¸åŒé‡‡æ ·å‚æ•°è¿½è¸ªGPUåˆ†é…çš„æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.server_manager = ServerManager()
        self.servers = []
        self.router = None
        
        # GPUç‰¹å®šçš„é‡‡æ ·å‚æ•°
        self.gpu_sampling_params = {
            0: {"temperature": 0.0, "top_p": 1.0},    # GPU 0: ç¡®å®šæ€§è¾“å‡º
            1: {"temperature": 1.0, "top_p": 0.95},   # GPU 1: éšæœºè¾“å‡º
        }
        
    async def setup_servers(self, model_path: str):
        """å¯åŠ¨å¸¦æœ‰ä¸åŒé‡‡æ ·å‚æ•°çš„æœåŠ¡å™¨"""
        
        # ä¸ºæ¯ä¸ªGPUå¯åŠ¨æœåŠ¡å™¨
        for gpu_id, sampling_params in self.gpu_sampling_params.items():
            server_config = {
                "server_id": f"gpu_{gpu_id}",
                "gpu_id": gpu_id,
                "port": 30001 + gpu_id,
                "model_path": model_path,
                "max_running_requests": 256,
                "mem_fraction_static": 0.9,
            }
            
            server = await self.server_manager.launch_server(server_config)
            self.servers.append(server)
            print(f"âœ… Started server on GPU {gpu_id} with params: {sampling_params}")
        
        # å¯åŠ¨router
        router_config = {
            "port": 30000,
            "policy": "round_robin",  # ä½¿ç”¨round_robinä¾¿äºéªŒè¯
            "prometheus_port": 29000,
        }
        
        worker_urls = [f"http://localhost:{s['port']}" for s in self.servers]
        self.router = await self.server_manager.launch_router(router_config, worker_urls)
        print(f"âœ… Started router with {len(worker_urls)} workers")
        
    async def send_test_request(self, prompt: str, request_id: str) -> Dict:
        """å‘é€æµ‹è¯•è¯·æ±‚å¹¶åˆ†æå“åº”"""
        
        # å‘é€ä¸¤æ¬¡ç›¸åŒçš„è¯·æ±‚åˆ°ä¸åŒçš„GPUï¼ˆé€šè¿‡å¤šæ¬¡å°è¯•ï¼‰
        responses = []
        
        for attempt in range(2):
            async with aiohttp.ClientSession() as session:
                # æ ¹æ®GPUä½¿ç”¨ä¸åŒçš„é‡‡æ ·å‚æ•°
                for gpu_id, sampling_params in self.gpu_sampling_params.items():
                    payload = {
                        "text": prompt,
                        "sampling_params": {
                            **sampling_params,
                            "max_new_tokens": 50,
                        },
                        "stream": False
                    }
                    
                    async with session.post(
                        "http://localhost:30000/generate",
                        json=payload
                    ) as response:
                        if response.status == 200:
                            result = await response.json()
                            responses.append({
                                "attempt": attempt,
                                "gpu_params": sampling_params,
                                "output": result.get("text", ""),
                                "request_id": f"{request_id}_attempt{attempt}"
                            })
        
        return self.analyze_responses(responses)
    
    def analyze_responses(self, responses: List[Dict]) -> Dict:
        """åˆ†æå“åº”ä»¥ç¡®å®šGPUåˆ†é…"""
        
        # å¯¹äºtemperature=0çš„GPUï¼Œè¾“å‡ºåº”è¯¥æ˜¯ç¡®å®šçš„
        # å¯¹äºtemperature=1çš„GPUï¼Œè¾“å‡ºåº”è¯¥æœ‰å˜åŒ–
        
        if len(responses) >= 2:
            output1 = responses[0]["output"]
            output2 = responses[1]["output"]
            
            # å¦‚æœä¸¤æ¬¡è¾“å‡ºå®Œå…¨ç›¸åŒï¼Œå¾ˆå¯èƒ½æ˜¯GPU 0 (temperature=0)
            if output1 == output2:
                return {"likely_gpu": 0, "confidence": "high", "reason": "deterministic output"}
            else:
                return {"likely_gpu": 1, "confidence": "high", "reason": "varied output"}
        
        return {"likely_gpu": -1, "confidence": "low", "reason": "insufficient data"}
    
    async def run_tracking_test(self, num_requests: int = 100):
        """è¿è¡Œå®Œæ•´çš„è¿½è¸ªæµ‹è¯•"""
        
        print("\nğŸ§ª Running GPU Tracking Test")
        print("=" * 60)
        
        # ç»Ÿè®¡ç»“æœ
        gpu_assignments = defaultdict(int)
        confidence_levels = defaultdict(int)
        
        # ç”Ÿæˆæµ‹è¯•prompt
        test_prompts = [
            "What is 2+2?",  # ç®€å•ç¡®å®šæ€§é—®é¢˜
            "Tell me a story about",  # åˆ›é€ æ€§é—®é¢˜
            "List three colors:",  # åŠç¡®å®šæ€§é—®é¢˜
        ]
        
        for i in range(num_requests):
            prompt = test_prompts[i % len(test_prompts)]
            result = await self.send_test_request(prompt, f"req_{i}")
            
            gpu_id = result["likely_gpu"]
            confidence = result["confidence"]
            
            if gpu_id >= 0:
                gpu_assignments[gpu_id] += 1
            confidence_levels[confidence] += 1
            
            if i % 10 == 0:
                print(f"Progress: {i+1}/{num_requests} requests processed")
        
        # æ‰“å°ç»“æœ
        print("\nğŸ“Š GPU Assignment Results:")
        total_assigned = sum(gpu_assignments.values())
        for gpu_id, count in sorted(gpu_assignments.items()):
            percentage = (count / total_assigned * 100) if total_assigned > 0 else 0
            print(f"  GPU {gpu_id}: {count} requests ({percentage:.1f}%)")
        
        print("\nğŸ¯ Confidence Levels:")
        for level, count in confidence_levels.items():
            print(f"  {level}: {count} requests")
        
        # éªŒè¯round-robinåˆ†å¸ƒ
        if len(gpu_assignments) == 2:
            counts = list(gpu_assignments.values())
            diff = abs(counts[0] - counts[1])
            print(f"\nğŸ“ˆ Balance Check: Difference = {diff} requests")
            if diff <= num_requests * 0.1:  # 10%å®¹å·®
                print("âœ… Distribution matches round-robin expectation!")
            else:
                print("âš ï¸  Distribution seems imbalanced")
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        await self.server_manager.stop_all()


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    
    model_path = "/data/pretrained_models/Llama-2-7b-hf"  # ä¿®æ”¹ä¸ºå®é™…è·¯å¾„
    
    test = GPUTrackingTest()
    
    try:
        # è®¾ç½®æœåŠ¡å™¨
        await test.setup_servers(model_path)
        
        # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
        await asyncio.sleep(5)
        
        # è¿è¡Œè¿½è¸ªæµ‹è¯•
        await test.run_tracking_test(num_requests=50)
        
    finally:
        # æ¸…ç†
        await test.cleanup()


if __name__ == "__main__":
    print("ğŸš€ GPU Tracking via Sampling Parameters")
    print("Strategy: GPU 0 uses temperature=0 (deterministic)")
    print("         GPU 1 uses temperature=1 (random)")
    print("=" * 60)
    
    asyncio.run(main())