#!/usr/bin/env python3
"""
é€šè¿‡ç”Ÿæˆå‚æ•°å·®å¼‚åŒ–æ¥è¿½è¸ªæ¯ä¸ªè¯·æ±‚çš„GPUåˆ†é…ã€‚
ä¸éœ€è¦ä¿®æ”¹routerï¼Œç«‹å³å¯ç”¨ã€‚
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from sglang_test_framework import RoutingTest, RoutingConfig
from sglang_test_framework.config.base import ServerConfig
import json
import re

class GPUTrackingRoutingConfig(RoutingConfig):
    """æ‰©å±•RoutingConfigä»¥æ”¯æŒper-GPUç”Ÿæˆå‚æ•°"""
    
    def _create_uniform_server_configs(self):
        """åˆ›å»ºå¸¦æœ‰ä¸åŒç”Ÿæˆå‚æ•°çš„æœåŠ¡å™¨é…ç½®"""
        configs = []
        
        # ä¸ºæ¯ä¸ªGPUè®¾ç½®ä¸åŒçš„æ ‡è¯†å‚æ•°
        gpu_params = {
            0: {
                "temperature": 0.7,
                "top_p": 0.9,
                "seed": 42,
            },
            1: {
                "temperature": 0.8,
                "top_p": 0.95,
                "seed": 43,
            }
        }
        
        for i, gpu_id in enumerate(self.gpu_ids):
            config = ServerConfig(
                server_id=f"worker_{i}",
                gpu_id=gpu_id,
                port=self.base_port + i,
                host=self.server_host,
                model_path=self.model_path,
                tokenizer_path=self.tokenizer_path,
                mem_fraction_static=self.mem_fraction_static,
                max_running_requests=self.max_running_requests,
                chunked_prefill_size=self.chunked_prefill_size,
                enable_torch_compile=self.enable_torch_compile,
                quantization=self.quantization,
                enable_metrics=True,
                verbose=True
            )
            
            # æ·»åŠ GPUç‰¹å®šçš„ç”Ÿæˆå‚æ•°ä½œä¸ºé¢å¤–å¯åŠ¨å‚æ•°
            # æ³¨æ„ï¼šè¿™éœ€è¦ç¡®è®¤SGLangæ˜¯å¦æ”¯æŒè¿™äº›å‚æ•°
            gpu_param = gpu_params.get(gpu_id, {})
            
            # æ–¹æ³•1ï¼šé€šè¿‡ç¯å¢ƒå˜é‡ä¼ é€’ï¼ˆå¦‚æœSGLangæ”¯æŒï¼‰
            # os.environ[f"SGLANG_GPU_{gpu_id}_TEMPERATURE"] = str(gpu_param["temperature"])
            
            # æ–¹æ³•2ï¼šä¿®æ”¹é»˜è®¤ç”Ÿæˆå‚æ•°ï¼ˆéœ€è¦ç¡®è®¤SGLang APIï¼‰
            # config.default_generation_params = gpu_param
            
            configs.append(config)
            
        return configs


def analyze_output_for_gpu(output_text: str, request_params: dict) -> int:
    """é€šè¿‡è¾“å‡ºç‰¹å¾æ¨æ–­GPU ID"""
    
    # æ–¹æ³•1ï¼šå¦‚æœæˆ‘ä»¬åœ¨promptä¸­æ³¨å…¥äº†æ ‡è¯†
    if "[GPU0]" in output_text:
        return 0
    elif "[GPU1]" in output_text:
        return 1
    
    # æ–¹æ³•2ï¼šé€šè¿‡è¾“å‡ºçš„éšæœºæ€§ç‰¹å¾æ¨æ–­ï¼ˆéœ€è¦ç»Ÿè®¡åˆ†æï¼‰
    # è¿™é‡Œéœ€è¦æ›´å¤æ‚çš„åˆ†æé€»è¾‘
    
    # æ–¹æ³•3ï¼šå¦‚æœæœåŠ¡å™¨åœ¨å“åº”ä¸­åŒ…å«äº†æ ‡è¯†
    # éœ€è¦æ£€æŸ¥å®Œæ•´çš„å“åº”ç»“æ„
    
    return -1  # æ— æ³•ç¡®å®š


def test_with_generation_tracking():
    """ä½¿ç”¨ç”Ÿæˆå‚æ•°å·®å¼‚åŒ–è¿›è¡ŒGPUè¿½è¸ªæµ‹è¯•"""
    
    # é…ç½®æµ‹è¯•
    config = GPUTrackingRoutingConfig(
        model_path="/data/pretrained_models/Llama-2-7b-hf",
        num_gpus=2,
        gpu_ids=[0, 1],
        routing_policy="round_robin",
        num_prompts=100,
        request_rate=20.0,
        dataset_name="random",
        random_input_len=256,
        random_output_len=64,
        output_dir="./gpu_tracking_results",
    )
    
    print("ğŸ” GPU Tracking via Generation Parameters")
    print("=" * 60)
    print("Strategy: Different generation parameters per GPU")
    print(f"GPU 0: temperature=0.7, seed=42")
    print(f"GPU 1: temperature=0.8, seed=43")
    print("=" * 60)
    
    # è¿è¡Œæµ‹è¯•
    test = RoutingTest(config)
    
    # ä¿®æ”¹è¯·æ±‚ç”Ÿæˆä»¥åŒ…å«GPUè¿½è¸ª
    # è¿™é‡Œæˆ‘ä»¬éœ€è¦è‡ªå®šä¹‰è¯·æ±‚å¤„ç†é€»è¾‘
    
    results = test.run()
    
    # åˆ†æç»“æœ
    print("\nğŸ“Š Results Analysis")
    print("-" * 60)
    
    # åŸºäºè¾“å‡ºç‰¹å¾ç»Ÿè®¡GPUåˆ†é…
    gpu_assignments = {0: 0, 1: 0, -1: 0}
    
    # è¿™é‡Œéœ€è¦è®¿é—®åŸå§‹å“åº”æ•°æ®
    # å®é™…å®ç°éœ€è¦ä¿®æ”¹RequestResultä»¥ä¿å­˜å®Œæ•´å“åº”
    
    print("\nGPU Assignment Statistics:")
    for gpu_id, count in sorted(gpu_assignments.items()):
        if gpu_id >= 0:
            print(f"  GPU {gpu_id}: {count} requests")
        else:
            print(f"  Unknown: {count} requests")


def simple_prompt_injection_test():
    """ç®€å•çš„promptæ³¨å…¥æ–¹æ³•æµ‹è¯•"""
    
    print("\nğŸ·ï¸  Simple Prompt Injection Method")
    print("=" * 60)
    
    # ä¸ºæ¯ä¸ªGPUåˆ›å»ºå¸¦æ ‡è¯†çš„system prompt
    gpu_prompts = {
        0: "You are a helpful assistant. [GPU0]",
        1: "You are a helpful assistant. [GPU1]"
    }
    
    # è¿™éœ€è¦åœ¨æœåŠ¡å™¨å¯åŠ¨æ—¶è®¾ç½®ä¸åŒçš„system prompt
    # æˆ–è€…åœ¨æ¯ä¸ªè¯·æ±‚ä¸­åŠ¨æ€æ³¨å…¥
    
    config = RoutingConfig(
        model_path="/data/pretrained_models/Llama-2-7b-hf",
        num_gpus=2,
        gpu_ids=[0, 1],
        routing_policy="round_robin",
        num_prompts=50,
        request_rate=10.0,
        dataset_name="random",
        random_input_len=128,
        random_output_len=32,
    )
    
    # ä¿®æ”¹è¯·æ±‚ä»¥åŒ…å«è¿½è¸ªé€»è¾‘
    # éœ€è¦è‡ªå®šä¹‰RequestGenerator
    
    print("Note: This method requires modifying server startup")
    print("to use different system prompts per GPU.")


if __name__ == "__main__":
    # æµ‹è¯•ä¸åŒçš„è¿½è¸ªæ–¹æ³•
    print("ğŸš€ Testing GPU Tracking Methods\n")
    
    # æ–¹æ³•1ï¼šç”Ÿæˆå‚æ•°å·®å¼‚åŒ–
    test_with_generation_tracking()
    
    # æ–¹æ³•2ï¼šPromptæ³¨å…¥
    simple_prompt_injection_test()
    
    print("\n" + "=" * 60)
    print("ğŸ’¡ Recommendations:")
    print("1. For immediate testing: Use prompt injection")
    print("2. For production: Implement proper request tracking in router")
    print("3. For analysis: Combine with Prometheus for validation")
    print("=" * 60)