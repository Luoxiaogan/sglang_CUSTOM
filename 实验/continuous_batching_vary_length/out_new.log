nohup: ignoring input
开始基准测试...
本次将测试QPS: 50 30 10 15 20 25 35 40 45 55 60 65 70 75 80 85 90 95 100 5
输入长度均值: 256, 输出长度均值: 64
将进行5次重复测试
----------------------------------------------------
正在生成测试数据集...
成功生成数据集，包含 2000 条记录，已保存到 /home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl
数据集生成完毕: /home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl
----------------------------------------------------

开始第 1 轮测试...

>>> 正在测试 Request Rate (QPS): 50 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=50.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_50.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=50.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_50.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
Warmup completed with 1 sequences. Starting main benchmark run...
  0%|          | 0/2000 [00:00<?, ?it/s]  0%|          | 1/2000 [00:00<10:41,  3.11it/s]  0%|          | 4/2000 [00:00<03:02, 10.92it/s]  0%|          | 6/2000 [00:00<02:54, 11.44it/s]  0%|          | 9/2000 [00:00<02:07, 15.59it/s]  1%|          | 17/2000 [00:00<01:03, 31.12it/s]  1%|          | 21/2000 [00:01<01:49, 18.02it/s]  1%|▏         | 25/2000 [00:01<01:43, 19.05it/s]  2%|▏         | 30/2000 [00:01<01:22, 23.96it/s]  2%|▏         | 34/2000 [00:01<01:41, 19.39it/s]  3%|▎         | 55/2000 [00:02<00:51, 37.95it/s]  3%|▎         | 60/2000 [00:02<00:50, 38.79it/s]  3%|▎         | 65/2000 [00:02<00:48, 39.61it/s]  4%|▎         | 70/2000 [00:02<01:05, 29.61it/s]  4%|▍         | 79/2000 [00:02<00:51, 37.27it/s]  4%|▍         | 84/2000 [00:03<01:03, 30.23it/s]  4%|▍         | 88/2000 [00:03<01:02, 30.71it/s]  5%|▍         | 97/2000 [00:03<00:46, 41.26it/s]  5%|▌         | 102/2000 [00:03<00:52, 36.18it/s]  5%|▌         | 107/2000 [00:03<01:01, 30.82it/s]  6%|▌         | 111/2000 [00:03<01:10, 26.79it/s]  6%|▋         | 127/2000 [00:04<00:39, 47.51it/s]  7%|▋         | 133/2000 [00:04<00:39, 47.70it/s]  7%|▋         | 139/2000 [00:04<00:47, 39.59it/s]  7%|▋         | 144/2000 [00:04<00:46, 40.00it/s]  7%|▋         | 149/2000 [00:04<00:51, 35.93it/s]  8%|▊         | 160/2000 [00:04<00:38, 48.13it/s]  8%|▊         | 166/2000 [00:05<00:44, 40.97it/s]  9%|▊         | 172/2000 [00:05<00:50, 36.34it/s]  9%|▉         | 182/2000 [00:05<00:37, 47.87it/s] 10%|▉         | 190/2000 [00:05<00:39, 45.99it/s] 10%|▉         | 196/2000 [00:05<00:40, 44.00it/s] 10%|█         | 201/2000 [00:05<00:48, 37.19it/s] 10%|█         | 206/2000 [00:06<00:47, 37.66it/s] 11%|█         | 211/2000 [00:06<00:55, 32.30it/s] 11%|█         | 215/2000 [00:06<00:56, 31.48it/s] 11%|█         | 219/2000 [00:06<01:08, 25.88it/s] 11%|█▏        | 229/2000 [00:06<00:50, 34.83it/s] 12%|█▏        | 233/2000 [00:07<01:11, 24.62it/s] 12%|█▏        | 242/2000 [00:07<00:52, 33.30it/s] 12%|█▏        | 249/2000 [00:07<00:47, 37.21it/s] 13%|█▎        | 254/2000 [00:07<00:56, 31.12it/s] 13%|█▎        | 263/2000 [00:07<00:47, 36.46it/s] 14%|█▎        | 274/2000 [00:08<00:35, 49.24it/s] 14%|█▍        | 281/2000 [00:08<00:38, 44.86it/s] 14%|█▍        | 287/2000 [00:08<00:38, 44.68it/s] 15%|█▍        | 293/2000 [00:08<00:42, 40.15it/s] 15%|█▍        | 298/2000 [00:08<00:48, 34.99it/s] 15%|█▌        | 304/2000 [00:08<00:42, 39.57it/s] 15%|█▌        | 309/2000 [00:09<00:48, 34.98it/s] 16%|█▌        | 313/2000 [00:09<01:06, 25.42it/s] 16%|█▌        | 317/2000 [00:09<01:00, 27.79it/s] 16%|█▌        | 321/2000 [00:09<00:59, 28.14it/s] 16%|█▋        | 325/2000 [00:09<01:01, 27.35it/s] 17%|█▋        | 335/2000 [00:09<00:40, 40.61it/s] 17%|█▋        | 342/2000 [00:10<00:39, 41.77it/s] 18%|█▊        | 356/2000 [00:10<00:28, 58.42it/s] 18%|█▊        | 363/2000 [00:10<00:31, 52.51it/s] 18%|█▊        | 369/2000 [00:10<00:41, 38.88it/s] 19%|█▉        | 377/2000 [00:10<00:36, 44.81it/s] 19%|█▉        | 388/2000 [00:10<00:27, 57.65it/s] 20%|█▉        | 395/2000 [00:11<00:35, 45.04it/s] 20%|██        | 409/2000 [00:11<00:26, 60.84it/s] 21%|██        | 417/2000 [00:11<00:25, 61.99it/s] 21%|██▏       | 425/2000 [00:11<00:32, 49.16it/s] 22%|██▏       | 431/2000 [00:11<00:41, 37.47it/s] 22%|██▏       | 440/2000 [00:11<00:34, 45.55it/s] 22%|██▏       | 447/2000 [00:12<00:38, 40.12it/s] 23%|██▎       | 454/2000 [00:12<00:36, 42.05it/s] 23%|██▎       | 459/2000 [00:12<00:39, 38.97it/s] 23%|██▎       | 464/2000 [00:12<00:40, 38.34it/s] 23%|██▎       | 469/2000 [00:12<00:54, 28.29it/s] 24%|██▎       | 474/2000 [00:13<00:48, 31.74it/s] 24%|██▍       | 481/2000 [00:13<01:01, 24.86it/s] 24%|██▍       | 489/2000 [00:13<00:46, 32.48it/s] 25%|██▍       | 494/2000 [00:13<00:52, 28.60it/s] 25%|██▌       | 508/2000 [00:14<00:41, 35.88it/s] 26%|██▌       | 513/2000 [00:14<00:44, 33.28it/s] 26%|██▌       | 523/2000 [00:14<00:35, 41.73it/s] 27%|██▋       | 534/2000 [00:14<00:37, 39.59it/s] 27%|██▋       | 545/2000 [00:15<00:40, 36.02it/s] 28%|██▊       | 551/2000 [00:15<00:38, 37.41it/s] 28%|██▊       | 556/2000 [00:15<00:42, 33.98it/s] 28%|██▊       | 568/2000 [00:15<00:36, 38.95it/s] 29%|██▉       | 583/2000 [00:15<00:27, 51.00it/s] 30%|██▉       | 594/2000 [00:16<00:26, 52.85it/s] 30%|███       | 602/2000 [00:16<00:26, 53.70it/s] 30%|███       | 609/2000 [00:16<00:25, 53.83it/s] 31%|███       | 615/2000 [00:16<00:30, 45.87it/s] 31%|███       | 620/2000 [00:16<00:31, 44.46it/s] 31%|███▏      | 628/2000 [00:16<00:27, 50.54it/s] 32%|███▏      | 634/2000 [00:17<00:38, 35.87it/s] 32%|███▏      | 639/2000 [00:17<00:39, 34.31it/s] 32%|███▏      | 647/2000 [00:17<00:33, 40.46it/s] 33%|███▎      | 653/2000 [00:17<00:41, 32.50it/s] 33%|███▎      | 657/2000 [00:17<00:40, 33.38it/s] 33%|███▎      | 661/2000 [00:17<00:40, 33.05it/s] 33%|███▎      | 665/2000 [00:18<00:41, 32.20it/s] 34%|███▎      | 670/2000 [00:18<00:37, 35.77it/s] 34%|███▍      | 683/2000 [00:18<00:23, 56.06it/s] 34%|███▍      | 690/2000 [00:18<00:25, 52.40it/s] 35%|███▍      | 696/2000 [00:18<00:25, 51.31it/s] 35%|███▌      | 706/2000 [00:18<00:28, 45.50it/s] 36%|███▌      | 714/2000 [00:18<00:27, 47.46it/s] 36%|███▌      | 722/2000 [00:19<00:27, 46.30it/s] 36%|███▋      | 729/2000 [00:19<00:26, 48.68it/s] 37%|███▋      | 739/2000 [00:19<00:25, 48.65it/s] 38%|███▊      | 751/2000 [00:19<00:22, 55.41it/s] 38%|███▊      | 757/2000 [00:19<00:23, 53.11it/s] 38%|███▊      | 763/2000 [00:19<00:25, 48.27it/s] 38%|███▊      | 768/2000 [00:20<00:28, 43.51it/s] 39%|███▊      | 773/2000 [00:20<00:33, 36.11it/s] 39%|███▉      | 779/2000 [00:20<00:31, 39.14it/s] 39%|███▉      | 789/2000 [00:20<00:23, 51.49it/s] 40%|███▉      | 795/2000 [00:20<00:23, 50.65it/s] 40%|████      | 803/2000 [00:20<00:27, 43.39it/s] 41%|████      | 811/2000 [00:21<00:27, 43.35it/s] 41%|████      | 816/2000 [00:21<00:30, 38.34it/s] 41%|████      | 821/2000 [00:21<00:46, 25.62it/s] 43%|████▎     | 866/2000 [00:21<00:15, 71.48it/s] 44%|████▎     | 874/2000 [00:22<00:16, 67.54it/s] 44%|████▍     | 882/2000 [00:22<00:17, 62.26it/s] 45%|████▍     | 894/2000 [00:22<00:17, 63.47it/s] 45%|████▌     | 901/2000 [00:22<00:24, 44.15it/s] 45%|████▌     | 906/2000 [00:22<00:27, 39.70it/s] 46%|████▌     | 911/2000 [00:23<00:28, 37.97it/s] 46%|████▌     | 915/2000 [00:23<00:31, 34.54it/s] 46%|████▌     | 922/2000 [00:23<00:27, 38.96it/s] 46%|████▋     | 928/2000 [00:23<00:27, 39.39it/s] 47%|████▋     | 938/2000 [00:23<00:25, 42.00it/s] 47%|████▋     | 949/2000 [00:23<00:19, 54.45it/s] 48%|████▊     | 957/2000 [00:24<00:22, 45.81it/s] 48%|████▊     | 966/2000 [00:24<00:28, 36.57it/s] 49%|████▊     | 972/2000 [00:24<00:39, 25.95it/s] 49%|████▉     | 979/2000 [00:25<00:34, 29.57it/s] 49%|████▉     | 988/2000 [00:25<00:28, 35.60it/s] 50%|████▉     | 994/2000 [00:25<00:28, 35.05it/s] 50%|████▉     | 999/2000 [00:25<00:26, 37.27it/s] 50%|█████     | 1004/2000 [00:25<00:25, 39.62it/s] 50%|█████     | 1010/2000 [00:25<00:24, 41.14it/s] 51%|█████     | 1018/2000 [00:25<00:22, 43.33it/s] 51%|█████▏    | 1026/2000 [00:26<00:19, 50.37it/s] 52%|█████▏    | 1032/2000 [00:26<00:21, 45.30it/s] 52%|█████▏    | 1039/2000 [00:26<00:19, 48.31it/s] 52%|█████▏    | 1047/2000 [00:26<00:20, 47.45it/s] 53%|█████▎    | 1056/2000 [00:26<00:17, 52.66it/s] 53%|█████▎    | 1062/2000 [00:26<00:19, 47.71it/s] 53%|█████▎    | 1067/2000 [00:26<00:19, 46.76it/s] 54%|█████▎    | 1073/2000 [00:27<00:33, 27.88it/s] 54%|█████▍    | 1084/2000 [00:27<00:23, 39.10it/s] 55%|█████▍    | 1090/2000 [00:27<00:29, 31.36it/s] 55%|█████▍    | 1095/2000 [00:28<00:32, 27.43it/s] 55%|█████▌    | 1107/2000 [00:28<00:21, 41.05it/s] 56%|█████▌    | 1113/2000 [00:28<00:20, 42.62it/s] 56%|█████▌    | 1119/2000 [00:28<00:29, 29.64it/s] 56%|█████▌    | 1124/2000 [00:28<00:34, 25.15it/s] 57%|█████▋    | 1132/2000 [00:29<00:30, 28.90it/s] 57%|█████▋    | 1136/2000 [00:29<00:29, 29.37it/s] 57%|█████▋    | 1146/2000 [00:29<00:21, 39.23it/s] 58%|█████▊    | 1151/2000 [00:29<00:20, 41.18it/s] 58%|█████▊    | 1156/2000 [00:29<00:20, 40.31it/s] 58%|█████▊    | 1163/2000 [00:29<00:19, 43.22it/s] 59%|█████▊    | 1171/2000 [00:29<00:17, 47.77it/s] 59%|█████▉    | 1178/2000 [00:29<00:15, 52.68it/s] 59%|█████▉    | 1184/2000 [00:30<00:17, 45.68it/s] 60%|█████▉    | 1192/2000 [00:30<00:19, 42.24it/s] 60%|█████▉    | 1197/2000 [00:30<00:19, 41.80it/s] 60%|██████    | 1205/2000 [00:30<00:18, 43.91it/s] 60%|██████    | 1210/2000 [00:30<00:20, 38.41it/s] 61%|██████    | 1214/2000 [00:31<00:22, 35.07it/s] 61%|██████▏   | 1225/2000 [00:31<00:17, 44.58it/s] 62%|██████▏   | 1236/2000 [00:31<00:17, 44.16it/s] 62%|██████▏   | 1246/2000 [00:31<00:14, 51.43it/s] 63%|██████▎   | 1252/2000 [00:31<00:20, 37.37it/s] 63%|██████▎   | 1259/2000 [00:31<00:18, 41.10it/s] 63%|██████▎   | 1266/2000 [00:32<00:18, 40.69it/s] 64%|██████▍   | 1278/2000 [00:32<00:13, 55.20it/s] 64%|██████▍   | 1285/2000 [00:32<00:12, 57.20it/s] 65%|██████▍   | 1292/2000 [00:32<00:15, 45.90it/s] 65%|██████▌   | 1302/2000 [00:32<00:13, 51.75it/s] 65%|██████▌   | 1308/2000 [00:32<00:13, 52.74it/s] 66%|██████▌   | 1314/2000 [00:33<00:17, 39.49it/s] 67%|██████▋   | 1336/2000 [00:33<00:12, 54.91it/s] 67%|██████▋   | 1344/2000 [00:33<00:13, 49.11it/s] 68%|██████▊   | 1350/2000 [00:33<00:15, 41.78it/s] 68%|██████▊   | 1359/2000 [00:34<00:14, 43.05it/s] 68%|██████▊   | 1364/2000 [00:34<00:19, 33.33it/s] 69%|██████▉   | 1379/2000 [00:34<00:14, 41.76it/s] 69%|██████▉   | 1387/2000 [00:34<00:13, 46.84it/s] 70%|██████▉   | 1395/2000 [00:34<00:12, 48.69it/s] 70%|███████   | 1403/2000 [00:34<00:11, 53.49it/s] 70%|███████   | 1409/2000 [00:35<00:13, 44.95it/s] 71%|███████   | 1414/2000 [00:35<00:13, 43.27it/s] 71%|███████   | 1421/2000 [00:35<00:13, 43.31it/s] 72%|███████▏  | 1433/2000 [00:35<00:10, 55.13it/s] 72%|███████▏  | 1440/2000 [00:35<00:10, 51.68it/s] 72%|███████▏  | 1446/2000 [00:35<00:11, 47.44it/s] 73%|███████▎  | 1457/2000 [00:36<00:11, 45.42it/s] 73%|███████▎  | 1464/2000 [00:36<00:11, 46.31it/s] 74%|███████▍  | 1477/2000 [00:36<00:10, 47.65it/s] 74%|███████▍  | 1484/2000 [00:36<00:11, 44.87it/s] 74%|███████▍  | 1489/2000 [00:36<00:11, 44.31it/s] 75%|███████▍  | 1495/2000 [00:37<00:11, 44.58it/s] 75%|███████▌  | 1504/2000 [00:37<00:10, 48.35it/s] 76%|███████▌  | 1513/2000 [00:37<00:08, 55.42it/s] 76%|███████▌  | 1519/2000 [00:37<00:10, 47.35it/s] 76%|███████▌  | 1524/2000 [00:37<00:11, 41.10it/s] 77%|███████▋  | 1534/2000 [00:37<00:09, 49.46it/s] 77%|███████▋  | 1540/2000 [00:37<00:09, 48.71it/s] 78%|███████▊  | 1551/2000 [00:38<00:07, 58.04it/s] 78%|███████▊  | 1563/2000 [00:38<00:06, 68.04it/s] 79%|███████▊  | 1571/2000 [00:38<00:06, 68.99it/s] 79%|███████▉  | 1579/2000 [00:38<00:07, 58.26it/s] 79%|███████▉  | 1586/2000 [00:38<00:08, 50.13it/s] 80%|███████▉  | 1592/2000 [00:38<00:11, 36.71it/s] 80%|███████▉  | 1597/2000 [00:39<00:11, 35.71it/s] 80%|████████  | 1602/2000 [00:39<00:11, 35.83it/s] 80%|████████  | 1609/2000 [00:39<00:10, 38.61it/s] 81%|████████  | 1616/2000 [00:39<00:09, 39.30it/s] 81%|████████  | 1623/2000 [00:39<00:09, 41.24it/s] 82%|████████▏ | 1636/2000 [00:39<00:06, 57.64it/s] 82%|████████▏ | 1643/2000 [00:40<00:09, 36.83it/s] 82%|████████▏ | 1648/2000 [00:40<00:13, 25.72it/s] 83%|████████▎ | 1655/2000 [00:40<00:12, 28.25it/s] 83%|████████▎ | 1663/2000 [00:41<00:09, 33.95it/s] 84%|████████▎ | 1671/2000 [00:41<00:08, 40.52it/s] 84%|████████▍ | 1677/2000 [00:41<00:10, 30.46it/s] 84%|████████▍ | 1682/2000 [00:41<00:09, 32.98it/s] 84%|████████▍ | 1687/2000 [00:41<00:09, 34.67it/s] 85%|████████▍ | 1698/2000 [00:41<00:06, 49.10it/s] 85%|████████▌ | 1705/2000 [00:41<00:05, 51.35it/s] 86%|████████▌ | 1715/2000 [00:42<00:08, 35.45it/s] 87%|████████▋ | 1747/2000 [00:42<00:03, 64.28it/s] 88%|████████▊ | 1755/2000 [00:42<00:04, 54.08it/s] 88%|████████▊ | 1761/2000 [00:42<00:04, 54.74it/s] 88%|████████▊ | 1767/2000 [00:43<00:05, 45.53it/s] 89%|████████▊ | 1772/2000 [00:43<00:05, 38.83it/s] 89%|████████▉ | 1788/2000 [00:43<00:03, 56.38it/s] 90%|████████▉ | 1795/2000 [00:43<00:03, 53.15it/s] 90%|█████████ | 1806/2000 [00:43<00:03, 57.91it/s] 91%|█████████ | 1814/2000 [00:44<00:03, 55.63it/s] 91%|█████████ | 1823/2000 [00:44<00:03, 53.59it/s] 92%|█████████▏| 1836/2000 [00:44<00:02, 66.16it/s] 93%|█████████▎| 1854/2000 [00:44<00:01, 88.11it/s] 93%|█████████▎| 1867/2000 [00:44<00:01, 96.94it/s] 94%|█████████▍| 1878/2000 [00:44<00:01, 96.98it/s] 94%|█████████▍| 1889/2000 [00:44<00:01, 85.96it/s] 95%|█████████▌| 1903/2000 [00:44<00:01, 96.92it/s] 96%|█████████▌| 1914/2000 [00:45<00:00, 90.18it/s] 96%|█████████▌| 1924/2000 [00:45<00:01, 72.35it/s] 97%|█████████▋| 1933/2000 [00:45<00:01, 61.52it/s] 97%|█████████▋| 1940/2000 [00:45<00:00, 63.04it/s] 97%|█████████▋| 1947/2000 [00:45<00:00, 62.66it/s] 98%|█████████▊| 1957/2000 [00:45<00:00, 69.15it/s] 98%|█████████▊| 1965/2000 [00:46<00:00, 48.76it/s] 99%|█████████▊| 1971/2000 [00:46<00:00, 31.26it/s] 99%|█████████▉| 1976/2000 [00:46<00:00, 31.97it/s] 99%|█████████▉| 1981/2000 [00:47<00:00, 22.80it/s] 99%|█████████▉| 1985/2000 [00:47<00:00, 15.64it/s] 99%|█████████▉| 1988/2000 [00:48<00:00, 13.58it/s]100%|█████████▉| 1991/2000 [00:48<00:00, 14.85it/s]100%|█████████▉| 1994/2000 [00:48<00:00, 13.88it/s]100%|█████████▉| 1996/2000 [00:49<00:00,  6.91it/s]100%|█████████▉| 1998/2000 [00:50<00:00,  4.38it/s]100%|█████████▉| 1999/2000 [00:51<00:00,  3.66it/s]100%|██████████| 2000/2000 [00:56<00:00,  1.02s/it]100%|██████████| 2000/2000 [00:56<00:00, 35.58it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang    
Traffic request rate:                    50.0      
Max request concurrency:                 250       
Successful requests:                     2000      
Benchmark duration (s):                  56.22     
Total input tokens:                      203033    
Total generated tokens:                  113158    
Total generated tokens (retokenized):    115184    
Request throughput (req/s):              35.57     
Input token throughput (tok/s):          3611.16   
Output token throughput (tok/s):         2012.64   
Total token throughput (tok/s):          5623.79   
Concurrency:                             132.16    
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   3715.20   
Median E2E Latency (ms):                 2239.18   
---------------Time to First Token----------------
Mean TTFT (ms):                          80.28     
Median TTFT (ms):                        71.31     
P99 TTFT (ms):                           375.86    
---------------Inter-Token Latency----------------
Mean ITL (ms):                           65.40     
Median ITL (ms):                         42.59     
P95 ITL (ms):                            197.97    
P99 ITL (ms):                            293.08    
Max ITL (ms):                            425.02    
----------Total Latency (incl. Queueing)----------
Mean Total Latency (ms):                 3718.12   
Median Total Latency (ms):               2241.89   
P99 Total Latency (ms):                  23843.03  
==================================================
测试完成，正在将结果追加到 /home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_report_20250701_221049.jsonl
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 30 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=30.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_30.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=30.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_30.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
Warmup completed with 1 sequences. Starting main benchmark run...
  0%|          | 0/2000 [00:00<?, ?it/s]  0%|          | 1/2000 [00:00<11:35,  2.87it/s]  0%|          | 3/2000 [00:00<04:23,  7.57it/s]  0%|          | 6/2000 [00:00<02:23, 13.86it/s]  0%|          | 10/2000 [00:00<01:40, 19.81it/s]  1%|          | 13/2000 [00:00<01:50, 18.04it/s]  1%|          | 16/2000 [00:01<01:58, 16.79it/s]  1%|          | 20/2000 [00:01<01:40, 19.65it/s]  1%|          | 24/2000 [00:01<01:26, 22.95it/s]  1%|▏         | 27/2000 [00:01<01:24, 23.39it/s]  2%|▏         | 30/2000 [00:01<01:23, 23.60it/s]  2%|▏         | 33/2000 [00:01<02:09, 15.14it/s]  2%|▏         | 39/2000 [00:02<01:29, 21.87it/s]  2%|▏         | 42/2000 [00:02<01:30, 21.54it/s]  2%|▏         | 48/2000 [00:02<01:07, 29.03it/s]  3%|▎         | 52/2000 [00:02<01:08, 28.38it/s]  3%|▎         | 56/2000 [00:02<01:08, 28.30it/s]  3%|▎         | 60/2000 [00:02<01:17, 24.88it/s]  3%|▎         | 65/2000 [00:02<01:07, 28.57it/s]  4%|▎         | 70/2000 [00:03<01:01, 31.43it/s]  4%|▎         | 74/2000 [00:03<00:59, 32.54it/s]  4%|▍         | 78/2000 [00:03<01:36, 19.87it/s]  4%|▍         | 84/2000 [00:03<01:12, 26.34it/s]  4%|▍         | 90/2000 [00:03<01:02, 30.69it/s]  5%|▍         | 94/2000 [00:03<00:59, 31.80it/s]  5%|▍         | 98/2000 [00:04<01:04, 29.69it/s]  5%|▌         | 103/2000 [00:04<01:01, 31.01it/s]  5%|▌         | 107/2000 [00:04<01:16, 24.74it/s]  6%|▌         | 110/2000 [00:04<01:28, 21.36it/s]  6%|▌         | 114/2000 [00:04<01:35, 19.82it/s]  6%|▌         | 118/2000 [00:05<01:21, 23.02it/s]  6%|▌         | 121/2000 [00:05<01:20, 23.24it/s]  6%|▋         | 128/2000 [00:05<00:59, 31.51it/s]  7%|▋         | 133/2000 [00:05<00:57, 32.73it/s]  7%|▋         | 140/2000 [00:05<00:56, 32.82it/s]  7%|▋         | 144/2000 [00:05<00:54, 33.92it/s]  7%|▋         | 148/2000 [00:05<01:03, 29.33it/s]  8%|▊         | 152/2000 [00:06<01:04, 28.64it/s]  8%|▊         | 155/2000 [00:06<01:08, 27.03it/s]  8%|▊         | 160/2000 [00:06<01:00, 30.58it/s]  8%|▊         | 166/2000 [00:06<00:54, 33.76it/s]  8%|▊         | 170/2000 [00:06<00:57, 31.89it/s]  9%|▊         | 174/2000 [00:06<01:12, 25.21it/s]  9%|▉         | 177/2000 [00:07<01:24, 21.70it/s]  9%|▉         | 180/2000 [00:07<01:21, 22.46it/s]  9%|▉         | 186/2000 [00:07<01:14, 24.39it/s] 10%|▉         | 192/2000 [00:07<01:07, 26.63it/s] 10%|▉         | 197/2000 [00:07<01:07, 26.73it/s] 10%|█         | 204/2000 [00:07<00:53, 33.35it/s] 10%|█         | 208/2000 [00:08<00:58, 30.86it/s] 11%|█         | 212/2000 [00:08<01:10, 25.42it/s] 11%|█         | 216/2000 [00:08<01:06, 26.67it/s] 11%|█         | 219/2000 [00:08<01:12, 24.52it/s] 11%|█▏        | 228/2000 [00:08<00:47, 37.50it/s] 12%|█▏        | 233/2000 [00:08<00:57, 30.70it/s] 12%|█▏        | 237/2000 [00:09<01:01, 28.84it/s] 12%|█▏        | 241/2000 [00:09<01:08, 25.64it/s] 12%|█▏        | 244/2000 [00:09<01:24, 20.73it/s] 12%|█▏        | 249/2000 [00:09<01:29, 19.59it/s] 13%|█▎        | 255/2000 [00:10<01:12, 24.05it/s] 13%|█▎        | 259/2000 [00:10<01:07, 25.77it/s] 13%|█▎        | 262/2000 [00:10<01:16, 22.57it/s] 13%|█▎        | 265/2000 [00:10<01:21, 21.16it/s] 13%|█▎        | 268/2000 [00:10<01:18, 22.11it/s] 14%|█▍        | 275/2000 [00:10<00:55, 31.22it/s] 14%|█▍        | 279/2000 [00:10<00:57, 29.70it/s] 14%|█▍        | 283/2000 [00:11<00:54, 31.52it/s] 14%|█▍        | 287/2000 [00:11<01:36, 17.76it/s] 14%|█▍        | 290/2000 [00:11<01:30, 18.81it/s] 15%|█▍        | 295/2000 [00:11<01:20, 21.22it/s] 15%|█▌        | 300/2000 [00:11<01:08, 24.72it/s] 15%|█▌        | 307/2000 [00:12<00:55, 30.31it/s] 16%|█▌        | 311/2000 [00:12<00:58, 29.06it/s] 16%|█▌        | 315/2000 [00:12<01:03, 26.34it/s] 16%|█▋        | 325/2000 [00:12<00:41, 40.67it/s] 17%|█▋        | 331/2000 [00:12<00:39, 41.90it/s] 17%|█▋        | 336/2000 [00:12<00:40, 40.62it/s] 17%|█▋        | 341/2000 [00:13<00:47, 34.86it/s] 17%|█▋        | 346/2000 [00:13<01:06, 24.93it/s] 18%|█▊        | 350/2000 [00:13<01:06, 24.78it/s] 18%|█▊        | 355/2000 [00:13<01:00, 27.13it/s] 18%|█▊        | 359/2000 [00:14<01:25, 19.14it/s] 18%|█▊        | 366/2000 [00:14<01:07, 24.34it/s] 18%|█▊        | 370/2000 [00:14<01:15, 21.48it/s] 19%|█▉        | 378/2000 [00:14<00:57, 28.29it/s] 19%|█▉        | 382/2000 [00:14<01:10, 23.00it/s] 19%|█▉        | 385/2000 [00:15<01:09, 23.29it/s] 20%|█▉        | 390/2000 [00:15<00:58, 27.60it/s] 20%|█▉        | 394/2000 [00:15<01:00, 26.47it/s] 20%|██        | 402/2000 [00:15<00:44, 35.59it/s] 20%|██        | 409/2000 [00:15<00:37, 42.50it/s] 21%|██        | 414/2000 [00:15<00:39, 40.36it/s] 21%|██        | 421/2000 [00:15<00:34, 45.79it/s] 21%|██▏       | 429/2000 [00:15<00:33, 47.15it/s] 22%|██▏       | 434/2000 [00:16<00:49, 31.33it/s] 22%|██▏       | 439/2000 [00:16<00:51, 30.59it/s] 22%|██▏       | 443/2000 [00:16<00:52, 29.69it/s] 22%|██▏       | 447/2000 [00:16<00:50, 30.54it/s] 23%|██▎       | 451/2000 [00:16<01:02, 24.78it/s] 23%|██▎       | 454/2000 [00:17<01:06, 23.37it/s] 23%|██▎       | 458/2000 [00:17<01:01, 25.13it/s] 23%|██▎       | 463/2000 [00:17<00:51, 29.98it/s] 23%|██▎       | 468/2000 [00:17<00:48, 31.28it/s] 24%|██▎       | 472/2000 [00:17<00:54, 27.95it/s] 24%|██▍       | 476/2000 [00:17<01:09, 21.78it/s] 24%|██▍       | 484/2000 [00:18<00:47, 31.94it/s] 24%|██▍       | 489/2000 [00:18<00:51, 29.34it/s] 25%|██▍       | 493/2000 [00:18<00:48, 31.37it/s] 25%|██▍       | 497/2000 [00:18<00:50, 29.97it/s] 25%|██▌       | 503/2000 [00:18<00:45, 33.13it/s] 25%|██▌       | 507/2000 [00:18<00:43, 34.15it/s] 26%|██▌       | 511/2000 [00:18<00:47, 31.37it/s] 26%|██▌       | 515/2000 [00:19<00:52, 28.26it/s] 26%|██▌       | 522/2000 [00:19<00:43, 34.34it/s] 26%|██▋       | 526/2000 [00:19<01:02, 23.65it/s] 26%|██▋       | 530/2000 [00:19<00:58, 25.33it/s] 27%|██▋       | 533/2000 [00:19<00:57, 25.71it/s] 27%|██▋       | 537/2000 [00:19<00:53, 27.51it/s] 27%|██▋       | 542/2000 [00:20<00:59, 24.53it/s] 27%|██▋       | 546/2000 [00:20<01:06, 21.97it/s] 28%|██▊       | 551/2000 [00:20<00:54, 26.78it/s] 28%|██▊       | 558/2000 [00:20<00:50, 28.28it/s] 28%|██▊       | 562/2000 [00:20<00:48, 29.64it/s] 28%|██▊       | 566/2000 [00:21<00:47, 30.00it/s] 28%|██▊       | 570/2000 [00:21<00:45, 31.71it/s] 29%|██▊       | 574/2000 [00:21<00:45, 31.33it/s] 29%|██▉       | 578/2000 [00:21<00:43, 32.85it/s] 29%|██▉       | 582/2000 [00:21<01:15, 18.74it/s] 29%|██▉       | 589/2000 [00:21<00:56, 24.94it/s] 30%|██▉       | 595/2000 [00:22<00:47, 29.82it/s] 30%|██▉       | 599/2000 [00:22<00:52, 26.51it/s] 30%|███       | 606/2000 [00:22<00:42, 32.60it/s] 31%|███       | 611/2000 [00:22<00:40, 34.16it/s] 31%|███       | 615/2000 [00:22<00:44, 30.89it/s] 31%|███       | 620/2000 [00:22<00:43, 31.61it/s] 31%|███       | 624/2000 [00:22<00:44, 30.61it/s] 32%|███▏      | 635/2000 [00:23<00:29, 46.76it/s] 32%|███▏      | 641/2000 [00:23<00:28, 48.20it/s] 32%|███▏      | 647/2000 [00:23<00:43, 31.32it/s] 33%|███▎      | 652/2000 [00:23<00:43, 31.27it/s] 33%|███▎      | 659/2000 [00:23<00:34, 38.34it/s] 33%|███▎      | 664/2000 [00:24<00:39, 33.87it/s] 33%|███▎      | 669/2000 [00:24<00:52, 25.49it/s] 34%|███▎      | 673/2000 [00:24<00:54, 24.48it/s] 34%|███▍      | 682/2000 [00:24<00:37, 35.06it/s] 34%|███▍      | 687/2000 [00:24<00:40, 32.41it/s] 35%|███▍      | 691/2000 [00:24<00:41, 31.36it/s] 35%|███▍      | 695/2000 [00:25<00:43, 30.27it/s] 35%|███▍      | 699/2000 [00:25<00:42, 30.72it/s] 35%|███▌      | 703/2000 [00:25<00:40, 31.75it/s] 35%|███▌      | 707/2000 [00:25<00:43, 29.93it/s] 36%|███▌      | 713/2000 [00:25<00:37, 33.91it/s] 36%|███▌      | 717/2000 [00:25<00:43, 29.22it/s] 36%|███▌      | 722/2000 [00:26<00:42, 29.81it/s] 36%|███▋      | 726/2000 [00:26<01:03, 20.12it/s] 36%|███▋      | 729/2000 [00:26<01:03, 19.98it/s] 37%|███▋      | 734/2000 [00:26<01:00, 20.78it/s] 37%|███▋      | 737/2000 [00:26<01:02, 20.15it/s] 37%|███▋      | 740/2000 [00:27<01:01, 20.61it/s] 37%|███▋      | 743/2000 [00:27<01:10, 17.87it/s] 37%|███▋      | 748/2000 [00:27<00:57, 21.77it/s] 38%|███▊      | 753/2000 [00:27<00:46, 26.71it/s] 38%|███▊      | 757/2000 [00:27<00:48, 25.80it/s] 38%|███▊      | 764/2000 [00:27<00:39, 31.33it/s] 38%|███▊      | 769/2000 [00:27<00:35, 34.41it/s] 39%|███▊      | 774/2000 [00:28<00:35, 34.53it/s] 39%|███▉      | 779/2000 [00:28<00:33, 36.08it/s] 39%|███▉      | 783/2000 [00:28<00:36, 33.25it/s] 39%|███▉      | 787/2000 [00:28<00:36, 32.92it/s] 40%|███▉      | 791/2000 [00:28<00:44, 27.22it/s] 40%|███▉      | 794/2000 [00:28<00:48, 24.93it/s] 40%|███▉      | 799/2000 [00:29<00:41, 29.00it/s] 40%|████      | 807/2000 [00:29<00:32, 36.50it/s] 41%|████      | 813/2000 [00:29<00:28, 41.29it/s] 41%|████      | 818/2000 [00:29<00:45, 26.26it/s] 41%|████      | 823/2000 [00:29<00:39, 29.89it/s] 41%|████▏     | 829/2000 [00:30<00:42, 27.80it/s] 42%|████▏     | 836/2000 [00:30<00:35, 33.00it/s] 42%|████▏     | 840/2000 [00:30<00:41, 27.62it/s] 42%|████▏     | 846/2000 [00:30<00:36, 31.23it/s] 43%|████▎     | 851/2000 [00:30<00:34, 33.05it/s] 43%|████▎     | 855/2000 [00:30<00:36, 31.59it/s] 43%|████▎     | 861/2000 [00:30<00:32, 35.47it/s] 43%|████▎     | 866/2000 [00:31<00:30, 37.51it/s] 44%|████▎     | 870/2000 [00:31<00:35, 31.80it/s] 44%|████▍     | 878/2000 [00:31<00:26, 42.24it/s] 44%|████▍     | 883/2000 [00:31<00:53, 21.01it/s] 44%|████▍     | 889/2000 [00:32<00:45, 24.31it/s] 45%|████▍     | 893/2000 [00:32<00:46, 23.86it/s] 45%|████▍     | 897/2000 [00:32<00:51, 21.58it/s] 45%|████▌     | 906/2000 [00:32<00:35, 30.80it/s] 46%|████▌     | 910/2000 [00:32<00:33, 32.25it/s] 46%|████▌     | 916/2000 [00:32<00:31, 34.96it/s] 46%|████▌     | 924/2000 [00:33<00:27, 38.73it/s] 46%|████▋     | 929/2000 [00:33<00:35, 29.95it/s] 47%|████▋     | 933/2000 [00:33<00:38, 27.46it/s] 47%|████▋     | 939/2000 [00:33<00:32, 32.64it/s] 47%|████▋     | 943/2000 [00:33<00:34, 30.99it/s] 48%|████▊     | 950/2000 [00:33<00:30, 33.88it/s] 48%|████▊     | 957/2000 [00:34<00:26, 39.46it/s] 48%|████▊     | 962/2000 [00:34<00:24, 41.68it/s] 48%|████▊     | 967/2000 [00:34<00:45, 22.68it/s] 49%|████▊     | 973/2000 [00:34<00:36, 28.24it/s] 49%|████▉     | 978/2000 [00:34<00:33, 30.94it/s] 49%|████▉     | 983/2000 [00:35<00:35, 28.97it/s] 49%|████▉     | 988/2000 [00:35<00:32, 31.40it/s] 50%|████▉     | 993/2000 [00:35<00:29, 33.79it/s] 50%|████▉     | 997/2000 [00:35<00:30, 32.39it/s] 50%|█████     | 1001/2000 [00:35<00:34, 28.86it/s] 50%|█████     | 1006/2000 [00:35<00:30, 33.01it/s] 50%|█████     | 1010/2000 [00:35<00:30, 31.99it/s] 51%|█████     | 1014/2000 [00:36<00:37, 26.09it/s] 51%|█████     | 1017/2000 [00:36<00:49, 19.82it/s] 51%|█████     | 1024/2000 [00:36<00:34, 28.41it/s] 51%|█████▏    | 1028/2000 [00:36<00:36, 26.99it/s] 52%|█████▏    | 1034/2000 [00:36<00:29, 33.08it/s] 52%|█████▏    | 1038/2000 [00:36<00:37, 25.68it/s] 52%|█████▏    | 1045/2000 [00:37<00:35, 26.77it/s] 52%|█████▏    | 1049/2000 [00:37<00:36, 26.09it/s] 53%|█████▎    | 1055/2000 [00:37<00:31, 30.08it/s] 53%|█████▎    | 1061/2000 [00:37<00:26, 35.65it/s] 53%|█████▎    | 1066/2000 [00:37<00:28, 32.97it/s] 54%|█████▎    | 1070/2000 [00:38<00:31, 29.72it/s] 54%|█████▍    | 1080/2000 [00:38<00:21, 42.21it/s] 54%|█████▍    | 1085/2000 [00:38<00:34, 26.33it/s] 55%|█████▍    | 1092/2000 [00:38<00:28, 31.34it/s] 55%|█████▍    | 1099/2000 [00:38<00:29, 30.22it/s] 55%|█████▌    | 1104/2000 [00:39<00:29, 29.98it/s] 55%|█████▌    | 1108/2000 [00:39<00:32, 27.78it/s] 56%|█████▌    | 1115/2000 [00:39<00:29, 29.98it/s] 56%|█████▌    | 1121/2000 [00:39<00:30, 28.76it/s] 56%|█████▋    | 1127/2000 [00:39<00:28, 31.10it/s] 57%|█████▋    | 1132/2000 [00:39<00:25, 34.27it/s] 57%|█████▋    | 1137/2000 [00:40<00:23, 36.15it/s] 57%|█████▋    | 1141/2000 [00:40<00:26, 32.60it/s] 57%|█████▋    | 1145/2000 [00:40<00:28, 29.95it/s] 57%|█████▋    | 1149/2000 [00:40<00:29, 28.77it/s] 58%|█████▊    | 1156/2000 [00:40<00:25, 32.85it/s] 58%|█████▊    | 1166/2000 [00:40<00:17, 46.75it/s] 59%|█████▊    | 1172/2000 [00:41<00:19, 42.57it/s] 59%|█████▉    | 1177/2000 [00:41<00:20, 40.01it/s] 59%|█████▉    | 1183/2000 [00:41<00:18, 44.27it/s] 59%|█████▉    | 1188/2000 [00:41<00:23, 35.07it/s] 60%|█████▉    | 1193/2000 [00:41<00:29, 27.65it/s] 60%|█████▉    | 1197/2000 [00:41<00:27, 29.66it/s] 60%|██████    | 1201/2000 [00:42<00:32, 24.91it/s] 60%|██████    | 1204/2000 [00:42<00:42, 18.73it/s] 60%|██████    | 1207/2000 [00:42<00:48, 16.38it/s] 60%|██████    | 1210/2000 [00:42<00:54, 14.49it/s] 61%|██████    | 1217/2000 [00:43<00:34, 22.52it/s] 61%|██████    | 1221/2000 [00:43<00:34, 22.65it/s] 61%|██████    | 1224/2000 [00:43<00:43, 17.72it/s] 61%|██████▏   | 1227/2000 [00:43<00:41, 18.80it/s] 62%|██████▏   | 1231/2000 [00:43<00:34, 22.21it/s] 62%|██████▏   | 1238/2000 [00:43<00:24, 31.54it/s] 62%|██████▏   | 1242/2000 [00:44<00:25, 30.05it/s] 62%|██████▏   | 1246/2000 [00:44<00:25, 29.06it/s] 62%|██████▎   | 1250/2000 [00:44<00:25, 29.20it/s] 63%|██████▎   | 1254/2000 [00:44<00:29, 25.18it/s] 63%|██████▎   | 1257/2000 [00:44<00:28, 25.81it/s] 63%|██████▎   | 1260/2000 [00:44<00:33, 21.81it/s] 63%|██████▎   | 1267/2000 [00:44<00:23, 31.01it/s] 64%|██████▎   | 1273/2000 [00:45<00:21, 34.41it/s] 64%|██████▍   | 1281/2000 [00:45<00:24, 29.41it/s] 64%|██████▍   | 1285/2000 [00:45<00:23, 31.06it/s] 64%|██████▍   | 1289/2000 [00:45<00:21, 32.73it/s] 65%|██████▍   | 1293/2000 [00:45<00:21, 32.39it/s] 65%|██████▌   | 1302/2000 [00:45<00:19, 36.31it/s] 65%|██████▌   | 1307/2000 [00:46<00:17, 38.67it/s] 66%|██████▌   | 1312/2000 [00:46<00:20, 34.20it/s] 66%|██████▌   | 1321/2000 [00:46<00:19, 35.53it/s] 66%|██████▋   | 1325/2000 [00:46<00:19, 34.59it/s] 67%|██████▋   | 1331/2000 [00:46<00:17, 39.11it/s] 67%|██████▋   | 1336/2000 [00:47<00:22, 29.28it/s] 67%|██████▋   | 1340/2000 [00:47<00:21, 30.32it/s] 67%|██████▋   | 1344/2000 [00:47<00:24, 26.30it/s] 67%|██████▋   | 1347/2000 [00:47<00:27, 23.97it/s] 68%|██████▊   | 1350/2000 [00:47<00:27, 23.71it/s] 68%|██████▊   | 1354/2000 [00:47<00:27, 23.13it/s] 68%|██████▊   | 1360/2000 [00:47<00:21, 29.28it/s] 68%|██████▊   | 1364/2000 [00:48<00:26, 24.38it/s] 69%|██████▊   | 1372/2000 [00:48<00:18, 34.86it/s] 69%|██████▉   | 1377/2000 [00:48<00:19, 31.17it/s] 69%|██████▉   | 1381/2000 [00:48<00:19, 31.07it/s] 69%|██████▉   | 1385/2000 [00:48<00:19, 31.15it/s] 69%|██████▉   | 1389/2000 [00:48<00:24, 25.42it/s] 70%|██████▉   | 1394/2000 [00:49<00:22, 26.86it/s] 70%|██████▉   | 1399/2000 [00:49<00:19, 31.23it/s] 70%|███████   | 1404/2000 [00:49<00:17, 34.18it/s] 70%|███████   | 1408/2000 [00:49<00:23, 25.13it/s] 71%|███████   | 1412/2000 [00:49<00:25, 23.17it/s] 71%|███████   | 1415/2000 [00:50<00:29, 19.88it/s] 71%|███████   | 1419/2000 [00:50<00:29, 19.94it/s] 71%|███████▏  | 1425/2000 [00:50<00:22, 25.56it/s] 71%|███████▏  | 1428/2000 [00:50<00:26, 21.60it/s] 72%|███████▏  | 1431/2000 [00:50<00:25, 22.47it/s] 72%|███████▏  | 1438/2000 [00:50<00:18, 30.94it/s] 72%|███████▏  | 1442/2000 [00:50<00:18, 30.04it/s] 72%|███████▏  | 1448/2000 [00:51<00:15, 36.67it/s] 73%|███████▎  | 1453/2000 [00:51<00:16, 33.78it/s] 73%|███████▎  | 1457/2000 [00:51<00:15, 35.03it/s] 73%|███████▎  | 1463/2000 [00:51<00:15, 34.82it/s] 73%|███████▎  | 1469/2000 [00:51<00:17, 30.09it/s] 74%|███████▎  | 1473/2000 [00:51<00:18, 28.89it/s] 74%|███████▍  | 1480/2000 [00:52<00:15, 34.08it/s] 74%|███████▍  | 1485/2000 [00:52<00:15, 32.37it/s] 74%|███████▍  | 1489/2000 [00:52<00:15, 32.46it/s] 75%|███████▍  | 1493/2000 [00:52<00:20, 25.34it/s] 75%|███████▍  | 1496/2000 [00:52<00:20, 24.55it/s] 75%|███████▌  | 1500/2000 [00:52<00:21, 23.15it/s] 75%|███████▌  | 1505/2000 [00:53<00:17, 27.97it/s] 75%|███████▌  | 1509/2000 [00:53<00:16, 30.19it/s] 76%|███████▌  | 1513/2000 [00:53<00:17, 27.07it/s] 76%|███████▌  | 1516/2000 [00:53<00:18, 26.52it/s] 76%|███████▌  | 1522/2000 [00:53<00:14, 32.38it/s] 76%|███████▋  | 1528/2000 [00:53<00:12, 38.53it/s] 77%|███████▋  | 1535/2000 [00:53<00:10, 45.91it/s] 77%|███████▋  | 1540/2000 [00:53<00:10, 42.10it/s] 77%|███████▋  | 1545/2000 [00:54<00:11, 39.09it/s] 78%|███████▊  | 1550/2000 [00:54<00:14, 30.58it/s] 78%|███████▊  | 1557/2000 [00:54<00:11, 37.00it/s] 78%|███████▊  | 1566/2000 [00:54<00:09, 43.62it/s] 79%|███████▊  | 1571/2000 [00:54<00:14, 30.19it/s] 79%|███████▉  | 1575/2000 [00:55<00:16, 26.32it/s] 79%|███████▉  | 1579/2000 [00:55<00:14, 28.63it/s] 79%|███████▉  | 1583/2000 [00:55<00:14, 28.41it/s] 79%|███████▉  | 1587/2000 [00:55<00:16, 25.75it/s] 80%|███████▉  | 1590/2000 [00:55<00:15, 26.42it/s] 80%|███████▉  | 1599/2000 [00:55<00:11, 33.57it/s] 80%|████████  | 1604/2000 [00:56<00:11, 35.74it/s] 80%|████████  | 1608/2000 [00:56<00:14, 26.92it/s] 81%|████████  | 1612/2000 [00:56<00:14, 27.35it/s] 81%|████████  | 1615/2000 [00:56<00:14, 26.01it/s] 81%|████████  | 1618/2000 [00:56<00:14, 26.83it/s] 81%|████████  | 1624/2000 [00:56<00:12, 30.36it/s] 81%|████████▏ | 1628/2000 [00:56<00:12, 29.40it/s] 82%|████████▏ | 1631/2000 [00:57<00:17, 21.38it/s] 82%|████████▏ | 1636/2000 [00:57<00:13, 26.20it/s] 82%|████████▏ | 1641/2000 [00:57<00:12, 28.39it/s] 82%|████████▏ | 1645/2000 [00:57<00:13, 26.01it/s] 82%|████████▏ | 1648/2000 [00:57<00:16, 21.60it/s] 83%|████████▎ | 1651/2000 [00:58<00:19, 17.90it/s] 83%|████████▎ | 1658/2000 [00:58<00:15, 22.72it/s] 83%|████████▎ | 1663/2000 [00:58<00:12, 26.54it/s] 83%|████████▎ | 1669/2000 [00:58<00:11, 29.69it/s] 84%|████████▎ | 1673/2000 [00:58<00:11, 29.48it/s] 84%|████████▍ | 1678/2000 [00:58<00:10, 31.52it/s] 84%|████████▍ | 1682/2000 [00:59<00:11, 27.82it/s] 84%|████████▍ | 1685/2000 [00:59<00:17, 17.81it/s] 84%|████████▍ | 1688/2000 [00:59<00:16, 19.18it/s] 85%|████████▍ | 1694/2000 [00:59<00:11, 26.32it/s] 85%|████████▍ | 1699/2000 [00:59<00:11, 27.06it/s] 85%|████████▌ | 1703/2000 [01:00<00:11, 26.12it/s] 85%|████████▌ | 1707/2000 [01:00<00:10, 26.87it/s] 86%|████████▌ | 1710/2000 [01:00<00:11, 25.75it/s] 86%|████████▌ | 1717/2000 [01:00<00:08, 35.37it/s] 86%|████████▌ | 1722/2000 [01:00<00:07, 38.04it/s] 86%|████████▋ | 1727/2000 [01:00<00:10, 27.09it/s] 87%|████████▋ | 1731/2000 [01:01<00:10, 25.49it/s] 87%|████████▋ | 1735/2000 [01:01<00:11, 22.65it/s] 87%|████████▋ | 1738/2000 [01:01<00:10, 23.83it/s] 87%|████████▋ | 1743/2000 [01:01<00:10, 23.92it/s] 87%|████████▋ | 1746/2000 [01:01<00:11, 21.85it/s] 87%|████████▋ | 1749/2000 [01:02<00:13, 18.31it/s] 88%|████████▊ | 1755/2000 [01:02<00:10, 23.11it/s] 88%|████████▊ | 1760/2000 [01:02<00:11, 21.27it/s] 88%|████████▊ | 1763/2000 [01:02<00:10, 21.82it/s] 88%|████████▊ | 1767/2000 [01:02<00:09, 24.87it/s] 89%|████████▊ | 1774/2000 [01:02<00:07, 29.32it/s] 89%|████████▉ | 1778/2000 [01:03<00:08, 26.04it/s] 89%|████████▉ | 1781/2000 [01:03<00:08, 26.66it/s] 89%|████████▉ | 1784/2000 [01:03<00:11, 19.51it/s] 89%|████████▉ | 1787/2000 [01:03<00:11, 18.36it/s] 90%|████████▉ | 1790/2000 [01:03<00:11, 17.74it/s] 90%|████████▉ | 1793/2000 [01:04<00:11, 17.57it/s] 90%|████████▉ | 1796/2000 [01:04<00:10, 19.06it/s] 90%|████████▉ | 1799/2000 [01:04<00:14, 13.83it/s] 90%|█████████ | 1802/2000 [01:04<00:12, 15.98it/s] 90%|█████████ | 1804/2000 [01:04<00:12, 16.27it/s] 91%|█████████ | 1811/2000 [01:04<00:07, 25.30it/s] 91%|█████████ | 1817/2000 [01:04<00:05, 32.20it/s] 91%|█████████ | 1821/2000 [01:05<00:06, 29.11it/s] 91%|█████████▏| 1827/2000 [01:05<00:05, 33.79it/s] 92%|█████████▏| 1831/2000 [01:05<00:06, 27.69it/s] 92%|█████████▏| 1835/2000 [01:05<00:05, 28.78it/s] 92%|█████████▏| 1839/2000 [01:05<00:05, 30.98it/s] 92%|█████████▏| 1845/2000 [01:05<00:04, 35.69it/s] 92%|█████████▏| 1849/2000 [01:06<00:05, 28.31it/s] 93%|█████████▎| 1863/2000 [01:06<00:02, 51.69it/s] 94%|█████████▎| 1870/2000 [01:06<00:02, 53.69it/s] 94%|█████████▍| 1877/2000 [01:06<00:03, 37.51it/s] 94%|█████████▍| 1883/2000 [01:06<00:03, 38.50it/s] 94%|█████████▍| 1888/2000 [01:07<00:05, 18.68it/s] 95%|█████████▌| 1906/2000 [01:07<00:02, 36.59it/s] 96%|█████████▌| 1914/2000 [01:07<00:02, 31.15it/s] 96%|█████████▌| 1920/2000 [01:08<00:02, 29.36it/s] 96%|█████████▋| 1925/2000 [01:08<00:03, 23.45it/s] 96%|█████████▋| 1929/2000 [01:08<00:03, 22.18it/s] 97%|█████████▋| 1933/2000 [01:08<00:02, 23.81it/s] 97%|█████████▋| 1938/2000 [01:09<00:02, 27.45it/s] 97%|█████████▋| 1942/2000 [01:09<00:03, 19.14it/s] 97%|█████████▋| 1945/2000 [01:09<00:02, 20.44it/s] 98%|█████████▊| 1950/2000 [01:09<00:01, 25.04it/s] 98%|█████████▊| 1954/2000 [01:09<00:01, 25.36it/s] 98%|█████████▊| 1958/2000 [01:10<00:01, 22.38it/s] 98%|█████████▊| 1964/2000 [01:10<00:01, 26.40it/s] 98%|█████████▊| 1970/2000 [01:10<00:00, 32.47it/s] 99%|█████████▉| 1975/2000 [01:10<00:00, 35.43it/s] 99%|█████████▉| 1980/2000 [01:10<00:00, 37.34it/s] 99%|█████████▉| 1985/2000 [01:10<00:00, 32.59it/s] 99%|█████████▉| 1989/2000 [01:10<00:00, 24.65it/s]100%|█████████▉| 1992/2000 [01:11<00:00, 15.38it/s]100%|█████████▉| 1995/2000 [01:11<00:00, 15.91it/s]100%|█████████▉| 1998/2000 [01:12<00:00,  9.81it/s]100%|██████████| 2000/2000 [01:15<00:00,  2.81it/s]100%|██████████| 2000/2000 [01:15<00:00, 26.62it/s]

============ Serving Benchmark Result ============
Backend:                                 sglang    
Traffic request rate:                    30.0      
Max request concurrency:                 250       
Successful requests:                     2000      
Benchmark duration (s):                  75.13     
Total input tokens:                      203033    
Total generated tokens:                  113158    
Total generated tokens (retokenized):    115184    
Request throughput (req/s):              26.62     
Input token throughput (tok/s):          2702.27   
Output token throughput (tok/s):         1506.08   
Total token throughput (tok/s):          4208.35   
Concurrency:                             36.20     
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   1359.94   
Median E2E Latency (ms):                 820.59    
---------------Time to First Token----------------
Mean TTFT (ms):                          56.75     
Median TTFT (ms):                        55.60     
P99 TTFT (ms):                           95.79     
---------------Inter-Token Latency----------------
Mean ITL (ms):                           23.45     
Median ITL (ms):                         15.12     
P95 ITL (ms):                            64.80     
P99 ITL (ms):                            123.72    
Max ITL (ms):                            417.36    
----------Total Latency (incl. Queueing)----------
Mean Total Latency (ms):                 1361.05   
Median Total Latency (ms):               822.14    
P99 Total Latency (ms):                  8955.90   
==================================================
测试完成，正在将结果追加到 /home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_report_20250701_221049.jsonl
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 10 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=10.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_10.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=10.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_10.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 1 轮 QPS 10 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 15 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=15.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_15.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=15.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_15.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 1 轮 QPS 15 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 20 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=20.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_20.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=20.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_20.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 1 轮 QPS 20 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 25 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=25.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_25.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=25.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_25.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 1 轮 QPS 25 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 35 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=35.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_35.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=35.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_35.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 1 轮 QPS 35 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 40 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=40.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_40.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=40.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_40.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 1 轮 QPS 40 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 45 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=45.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_45.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=45.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_45.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 1 轮 QPS 45 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 55 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=55.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_55.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=55.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_55.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 1 轮 QPS 55 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 60 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=60.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_60.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=60.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_60.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 1 轮 QPS 60 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 65 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=65.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_65.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=65.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_65.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 1 轮 QPS 65 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 70 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=70.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_70.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=70.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_70.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 1 轮 QPS 70 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 75 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=75.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_75.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=75.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_75.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 1 轮 QPS 75 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 80 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=80.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_80.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=80.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_80.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 1 轮 QPS 80 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 85 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=85.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_85.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=85.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_85.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 1 轮 QPS 85 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 90 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=90.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_90.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=90.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_90.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 1 轮 QPS 90 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 95 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=95.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_95.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=95.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_95.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 1 轮 QPS 95 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 100 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=100.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_100.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=100.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_100.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 1 轮 QPS 100 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 5 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=5.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_5.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=5.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_1_5.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 1 轮 QPS 5 未能生成结果文件，跳过处理。
----------------------------------------------------
第 1 轮测试完成
============================================================

开始第 2 轮测试...

>>> 正在测试 Request Rate (QPS): 50 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=50.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_50.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=50.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_50.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 2 轮 QPS 50 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 30 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=30.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_30.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=30.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_30.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 2 轮 QPS 30 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 10 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=10.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_10.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=10.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_10.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 2 轮 QPS 10 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 15 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=15.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_15.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=15.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_15.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 2 轮 QPS 15 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 20 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=20.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_20.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=20.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_20.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 2 轮 QPS 20 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 25 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=25.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_25.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=25.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_25.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 2 轮 QPS 25 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 35 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=35.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_35.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=35.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_35.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 2 轮 QPS 35 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 40 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=40.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_40.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=40.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_40.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 2 轮 QPS 40 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 45 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=45.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_45.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=45.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_45.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 2 轮 QPS 45 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 55 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=55.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_55.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=55.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_55.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 2 轮 QPS 55 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 60 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=60.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_60.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=60.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_60.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 2 轮 QPS 60 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 65 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=65.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_65.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=65.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_65.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 2 轮 QPS 65 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 70 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=70.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_70.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=70.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_70.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 2 轮 QPS 70 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 75 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=75.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_75.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=75.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_75.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 2 轮 QPS 75 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 80 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=80.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_80.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=80.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_80.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 2 轮 QPS 80 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 85 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=85.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_85.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=85.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_85.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 2 轮 QPS 85 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 90 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=90.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_90.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=90.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_90.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 2 轮 QPS 90 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 95 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=95.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_95.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=95.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_95.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 2 轮 QPS 95 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 100 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=100.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_100.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=100.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_100.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 2 轮 QPS 100 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 5 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=5.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_5.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=5.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_2_5.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 2 轮 QPS 5 未能生成结果文件，跳过处理。
----------------------------------------------------
第 2 轮测试完成
============================================================

开始第 3 轮测试...

>>> 正在测试 Request Rate (QPS): 50 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=50.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_50.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=50.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_50.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 3 轮 QPS 50 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 30 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=30.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_30.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=30.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_30.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 3 轮 QPS 30 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 10 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=10.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_10.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=10.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_10.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 3 轮 QPS 10 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 15 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=15.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_15.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=15.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_15.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 3 轮 QPS 15 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 20 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=20.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_20.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=20.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_20.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 3 轮 QPS 20 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 25 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=25.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_25.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=25.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_25.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 3 轮 QPS 25 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 35 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=35.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_35.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=35.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_35.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 3 轮 QPS 35 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 40 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=40.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_40.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=40.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_40.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 3 轮 QPS 40 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 45 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=45.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_45.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=45.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_45.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 3 轮 QPS 45 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 55 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=55.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_55.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=55.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_55.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 3 轮 QPS 55 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 60 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=60.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_60.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=60.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_60.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 3 轮 QPS 60 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 65 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=65.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_65.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=65.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_65.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 3 轮 QPS 65 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 70 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=70.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_70.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=70.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_70.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 3 轮 QPS 70 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 75 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=75.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_75.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=75.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_75.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 3 轮 QPS 75 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 80 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=80.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_80.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=80.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_80.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 3 轮 QPS 80 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 85 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=85.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_85.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=85.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_85.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 3 轮 QPS 85 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 90 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=90.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_90.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=90.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_90.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 3 轮 QPS 90 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 95 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=95.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_95.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=95.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_95.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 3 轮 QPS 95 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 100 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=100.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_100.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=100.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_100.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 3 轮 QPS 100 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 5 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=5.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_5.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=5.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_3_5.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 3 轮 QPS 5 未能生成结果文件，跳过处理。
----------------------------------------------------
第 3 轮测试完成
============================================================

开始第 4 轮测试...

>>> 正在测试 Request Rate (QPS): 50 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=50.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_50.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=50.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_50.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 4 轮 QPS 50 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 30 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=30.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_30.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=30.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_30.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 4 轮 QPS 30 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 10 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=10.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_10.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=10.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_10.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 4 轮 QPS 10 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 15 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=15.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_15.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=15.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_15.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 4 轮 QPS 15 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 20 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=20.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_20.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=20.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_20.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 4 轮 QPS 20 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 25 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=25.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_25.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=25.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_25.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 4 轮 QPS 25 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 35 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=35.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_35.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=35.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_35.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 4 轮 QPS 35 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 40 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=40.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_40.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=40.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_40.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 4 轮 QPS 40 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 45 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=45.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_45.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=45.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_45.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 4 轮 QPS 45 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 55 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=55.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_55.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=55.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_55.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 4 轮 QPS 55 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 60 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=60.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_60.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=60.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_60.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 4 轮 QPS 60 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 65 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=65.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_65.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=65.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_65.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 4 轮 QPS 65 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 70 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=70.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_70.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=70.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_70.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 4 轮 QPS 70 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 75 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=75.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_75.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=75.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_75.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 4 轮 QPS 75 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 80 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=80.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_80.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=80.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_80.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 4 轮 QPS 80 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 85 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=85.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_85.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=85.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_85.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 4 轮 QPS 85 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 90 ...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1982, in <module>
    run_benchmark(args)
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1732, in run_benchmark
    return asyncio.run(
           ^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 1294, in benchmark
    raise ValueError(
ValueError: Warmup failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait
    await waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate
    async for chunk_bytes in response.content:
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__
    rv = await self.read_func()
         ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline
    return await self.readuntil()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil
    await self._wait("readuntil")
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait
    with self._timer:
         ^^^^^^^^^^^
  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__
    raise asyncio.TimeoutError from exc_val
TimeoutError

benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=90.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_90.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

WARNING It is recommended to use the `Chat` or `Instruct` model for benchmarking.
Because when the tokenizer counts the output tokens, if there is gibberish, it might count incorrectly.

Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model='/data/pretrained_models/Llama-2-7b-hf', tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=90.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_90.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)

#Input tokens: 203033
#Output tokens: 113158
Starting warmup with 1 sequences...
output.error='Traceback (most recent call last):\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 347, in _wait\n    await waiter\nasyncio.exceptions.CancelledError\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/home/lg/sglang/python/sglang/bench_serving_new.py", line 390, in async_request_sglang_generate\n    async for chunk_bytes in response.content:\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 52, in __anext__\n    rv = await self.read_func()\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 352, in readline\n    return await self.readuntil()\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 386, in readuntil\n    await self._wait("readuntil")\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/streams.py", line 346, in _wait\n    with self._timer:\n         ^^^^^^^^^^^\n  File "/home/lg/.local/lib/python3.12/site-packages/aiohttp/helpers.py", line 685, in __exit__\n    raise asyncio.TimeoutError from exc_val\nTimeoutError\n'
警告: 第 4 轮 QPS 90 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 95 ...
Terminated
警告: 第 4 轮 QPS 95 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 100 ...
/home/lg/sglang/实验/continuous_batching_vary_length/run_benchmark_new.sh: line 47: 2765231 Killed                  python3 -m sglang.bench_serving_new --backend sglang --host "0.0.0.0" --port 33710 --tokenizer "$MODEL_TOKENIZER_PATH" --dataset-path "$DATASET_FILE" --request-rate "$RATE" --num-prompts "$NUM_PROMPTS" --output-file "$CURRENT_TMP_JSON" --max-concurrency 250
警告: 第 4 轮 QPS 100 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 5 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=5.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_4_5.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Failed to fetch model from http://0.0.0.0:33710/v1/models. Error: HTTPConnectionPool(host='0.0.0.0', port=33710): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a5f2fabd910>: Failed to establish a new connection: [Errno 111] Connection refused'))
Please specify the correct host and port using `--host` and `--port`.
警告: 第 4 轮 QPS 5 未能生成结果文件，跳过处理。
----------------------------------------------------
第 4 轮测试完成
============================================================

开始第 5 轮测试...

>>> 正在测试 Request Rate (QPS): 50 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=50.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_5_50.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Failed to fetch model from http://0.0.0.0:33710/v1/models. Error: HTTPConnectionPool(host='0.0.0.0', port=33710): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x75b548524260>: Failed to establish a new connection: [Errno 111] Connection refused'))
Please specify the correct host and port using `--host` and `--port`.
警告: 第 5 轮 QPS 50 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 30 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=30.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_5_30.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Failed to fetch model from http://0.0.0.0:33710/v1/models. Error: HTTPConnectionPool(host='0.0.0.0', port=33710): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x78af11d8c260>: Failed to establish a new connection: [Errno 111] Connection refused'))
Please specify the correct host and port using `--host` and `--port`.
警告: 第 5 轮 QPS 30 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 10 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=10.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_5_10.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Failed to fetch model from http://0.0.0.0:33710/v1/models. Error: HTTPConnectionPool(host='0.0.0.0', port=33710): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b43d6d4d400>: Failed to establish a new connection: [Errno 111] Connection refused'))
Please specify the correct host and port using `--host` and `--port`.
警告: 第 5 轮 QPS 10 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 15 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=15.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_5_15.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Failed to fetch model from http://0.0.0.0:33710/v1/models. Error: HTTPConnectionPool(host='0.0.0.0', port=33710): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7bb39db232c0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Please specify the correct host and port using `--host` and `--port`.
警告: 第 5 轮 QPS 15 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 20 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=20.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_5_20.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Failed to fetch model from http://0.0.0.0:33710/v1/models. Error: HTTPConnectionPool(host='0.0.0.0', port=33710): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x76e0ac764f80>: Failed to establish a new connection: [Errno 111] Connection refused'))
Please specify the correct host and port using `--host` and `--port`.
警告: 第 5 轮 QPS 20 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 25 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=25.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_5_25.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Failed to fetch model from http://0.0.0.0:33710/v1/models. Error: HTTPConnectionPool(host='0.0.0.0', port=33710): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c47d9a96a80>: Failed to establish a new connection: [Errno 111] Connection refused'))
Please specify the correct host and port using `--host` and `--port`.
警告: 第 5 轮 QPS 25 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 35 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=35.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_5_35.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Failed to fetch model from http://0.0.0.0:33710/v1/models. Error: HTTPConnectionPool(host='0.0.0.0', port=33710): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7e947629b2c0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Please specify the correct host and port using `--host` and `--port`.
警告: 第 5 轮 QPS 35 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 40 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=40.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_5_40.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Failed to fetch model from http://0.0.0.0:33710/v1/models. Error: HTTPConnectionPool(host='0.0.0.0', port=33710): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c9ee706ddf0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Please specify the correct host and port using `--host` and `--port`.
警告: 第 5 轮 QPS 40 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 45 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=45.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_5_45.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Failed to fetch model from http://0.0.0.0:33710/v1/models. Error: HTTPConnectionPool(host='0.0.0.0', port=33710): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7a7015dfa270>: Failed to establish a new connection: [Errno 111] Connection refused'))
Please specify the correct host and port using `--host` and `--port`.
警告: 第 5 轮 QPS 45 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 55 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=55.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_5_55.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Failed to fetch model from http://0.0.0.0:33710/v1/models. Error: HTTPConnectionPool(host='0.0.0.0', port=33710): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7013f41bcef0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Please specify the correct host and port using `--host` and `--port`.
警告: 第 5 轮 QPS 55 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 60 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=60.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_5_60.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Failed to fetch model from http://0.0.0.0:33710/v1/models. Error: HTTPConnectionPool(host='0.0.0.0', port=33710): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7773f31db2c0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Please specify the correct host and port using `--host` and `--port`.
警告: 第 5 轮 QPS 60 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 65 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=65.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_5_65.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Failed to fetch model from http://0.0.0.0:33710/v1/models. Error: HTTPConnectionPool(host='0.0.0.0', port=33710): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fddf2c428d0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Please specify the correct host and port using `--host` and `--port`.
警告: 第 5 轮 QPS 65 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 70 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=70.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_5_70.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Failed to fetch model from http://0.0.0.0:33710/v1/models. Error: HTTPConnectionPool(host='0.0.0.0', port=33710): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7dd52ef48980>: Failed to establish a new connection: [Errno 111] Connection refused'))
Please specify the correct host and port using `--host` and `--port`.
警告: 第 5 轮 QPS 70 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 75 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=75.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_5_75.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Failed to fetch model from http://0.0.0.0:33710/v1/models. Error: HTTPConnectionPool(host='0.0.0.0', port=33710): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7170b7855370>: Failed to establish a new connection: [Errno 111] Connection refused'))
Please specify the correct host and port using `--host` and `--port`.
警告: 第 5 轮 QPS 75 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 80 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=80.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_5_80.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Failed to fetch model from http://0.0.0.0:33710/v1/models. Error: HTTPConnectionPool(host='0.0.0.0', port=33710): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7daf99de8ef0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Please specify the correct host and port using `--host` and `--port`.
警告: 第 5 轮 QPS 80 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 85 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=85.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_5_85.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Failed to fetch model from http://0.0.0.0:33710/v1/models. Error: HTTPConnectionPool(host='0.0.0.0', port=33710): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x751682f19c10>: Failed to establish a new connection: [Errno 111] Connection refused'))
Please specify the correct host and port using `--host` and `--port`.
警告: 第 5 轮 QPS 85 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 90 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=90.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_5_90.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Failed to fetch model from http://0.0.0.0:33710/v1/models. Error: HTTPConnectionPool(host='0.0.0.0', port=33710): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7c1505931f40>: Failed to establish a new connection: [Errno 111] Connection refused'))
Please specify the correct host and port using `--host` and `--port`.
警告: 第 5 轮 QPS 90 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 95 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=95.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_5_95.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Failed to fetch model from http://0.0.0.0:33710/v1/models. Error: HTTPConnectionPool(host='0.0.0.0', port=33710): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x75bccc62a6f0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Please specify the correct host and port using `--host` and `--port`.
警告: 第 5 轮 QPS 95 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 100 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=100.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_5_100.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Failed to fetch model from http://0.0.0.0:33710/v1/models. Error: HTTPConnectionPool(host='0.0.0.0', port=33710): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x74a95c9f18e0>: Failed to establish a new connection: [Errno 111] Connection refused'))
Please specify the correct host and port using `--host` and `--port`.
警告: 第 5 轮 QPS 100 未能生成结果文件，跳过处理。
----------------------------------------------------

>>> 正在测试 Request Rate (QPS): 5 ...
benchmark_args=Namespace(backend='sglang', base_url=None, host='0.0.0.0', port=33710, dataset_name='sharegpt', dataset_path='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_dataset_20250701_221049.jsonl', model=None, tokenizer='/data/pretrained_models/Llama-2-7b-hf', num_prompts=2000, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=1024, random_output_len=1024, random_range_ratio=0.0, request_rate=5.0, max_concurrency=250, output_file='/home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/tmp_run_results_5_5.json', output_details=False, disable_tqdm=False, disable_stream=False, return_logprob=False, seed=1, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=False, lora_name=None, prompt_suffix='', pd_separated=False, flush_cache=False, warmup_requests=1, tokenize_prompt=False, gsp_num_groups=64, gsp_prompts_per_group=16, gsp_system_prompt_len=2048, gsp_question_len=128, gsp_output_len=256)
Failed to fetch model from http://0.0.0.0:33710/v1/models. Error: HTTPConnectionPool(host='0.0.0.0', port=33710): Max retries exceeded with url: /v1/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x73566134ff80>: Failed to establish a new connection: [Errno 111] Connection refused'))
Please specify the correct host and port using `--host` and `--port`.
警告: 第 5 轮 QPS 5 未能生成结果文件，跳过处理。
----------------------------------------------------
第 5 轮测试完成
============================================================
所有测试完成。正在清理临时文件...
基准测试全部完成！
最终报告保存在: /home/lg/sglang/实验/continuous_batching_vary_length/results_std_1_new--max-concurrency/benchmark_report_20250701_221049.jsonl
