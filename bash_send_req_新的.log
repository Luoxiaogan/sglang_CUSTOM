==========================================
SGLang Router 测试配置
==========================================
请求数量: 200
请求速率: 50.0 req/s
数据集: random
输入长度: 512 ±256%
输出长度: 128 ±64%
路由器地址: http://localhost:60009
==========================================

执行命令:
python /nas/ganluo/sglang/send_req.py --num-requests 200 --request-rate 50.0 --dataset random --router-url http://localhost:60009 --input-len 512 --output-len 128 --range-ratio 0.5

预计测试时间: 4.0秒（不含处理时间）

Generating 200 random requests...
✅ Router health check passed

Sending 200 requests at 50.0 req/s...
📤 Request 1/200 sent at 0.00s
📤 Request 2/200 sent at 0.01s
📤 Request 3/200 sent at 0.03s
📤 Request 4/200 sent at 0.03s
📤 Request 5/200 sent at 0.04s
📤 Request 6/200 sent at 0.05s
📤 Request 7/200 sent at 0.06s
📤 Request 8/200 sent at 0.06s
📤 Request 9/200 sent at 0.09s
📤 Request 10/200 sent at 0.10s
📤 Request 11/200 sent at 0.10s
📤 Request 12/200 sent at 0.12s
📤 Request 13/200 sent at 0.13s
📤 Request 14/200 sent at 0.13s
📤 Request 15/200 sent at 0.19s
📤 Request 16/200 sent at 0.21s
📤 Request 17/200 sent at 0.22s
📤 Request 18/200 sent at 0.25s
📤 Request 19/200 sent at 0.29s
📤 Request 20/200 sent at 0.29s
📤 Request 21/200 sent at 0.32s
📤 Request 22/200 sent at 0.35s
📤 Request 23/200 sent at 0.36s
📤 Request 24/200 sent at 0.37s
📤 Request 25/200 sent at 0.38s
📤 Request 26/200 sent at 0.40s
📤 Request 27/200 sent at 0.43s
📤 Request 28/200 sent at 0.46s
📤 Request 29/200 sent at 0.47s
📤 Request 30/200 sent at 0.51s
📤 Request 31/200 sent at 0.52s
📤 Request 32/200 sent at 0.53s
📤 Request 33/200 sent at 0.56s
📤 Request 34/200 sent at 0.60s
📤 Request 35/200 sent at 0.60s
📤 Request 36/200 sent at 0.65s
📤 Request 37/200 sent at 0.65s
📤 Request 38/200 sent at 0.68s
📤 Request 39/200 sent at 0.70s
📤 Request 40/200 sent at 0.72s
📤 Request 41/200 sent at 0.72s
📤 Request 42/200 sent at 0.72s
📤 Request 43/200 sent at 0.73s
📤 Request 44/200 sent at 0.82s
📤 Request 45/200 sent at 0.86s
📤 Request 46/200 sent at 0.87s
📤 Request 47/200 sent at 0.88s
📤 Request 48/200 sent at 0.92s
📤 Request 49/200 sent at 0.97s
📤 Request 50/200 sent at 1.01s
📤 Request 51/200 sent at 1.02s
📤 Request 52/200 sent at 1.03s
✅ Request 30/200 completed
✅ Request 16/200 completed
✅ Request 2/200 completed
📤 Request 53/200 sent at 1.15s
📤 Request 54/200 sent at 1.16s
📤 Request 55/200 sent at 1.17s
📤 Request 56/200 sent at 1.17s
📤 Request 57/200 sent at 1.18s
📤 Request 58/200 sent at 1.21s
✅ Request 5/200 completed
📤 Request 59/200 sent at 1.23s
✅ Request 19/200 completed
📤 Request 60/200 sent at 1.25s
✅ Request 9/200 completed
✅ Request 1/200 completed
✅ Request 42/200 completed
📤 Request 61/200 sent at 1.32s
✅ Request 3/200 completed
📤 Request 62/200 sent at 1.34s
📤 Request 63/200 sent at 1.35s
📤 Request 64/200 sent at 1.40s
✅ Request 24/200 completed
✅ Request 35/200 completed
✅ Request 10/200 completed
✅ Request 21/200 completed
✅ Request 45/200 completed
📤 Request 65/200 sent at 1.48s
📤 Request 66/200 sent at 1.50s
✅ Request 6/200 completed
📤 Request 67/200 sent at 1.51s
✅ Request 14/200 completed
📤 Request 68/200 sent at 1.52s
✅ Request 27/200 completed
📤 Request 69/200 sent at 1.57s
📤 Request 70/200 sent at 1.59s
📤 Request 71/200 sent at 1.59s
📤 Request 72/200 sent at 1.60s
✅ Request 17/200 completed
📤 Request 73/200 sent at 1.65s
✅ Request 26/200 completed
✅ Request 18/200 completed
📤 Request 74/200 sent at 1.69s
📤 Request 75/200 sent at 1.74s
📤 Request 76/200 sent at 1.77s
📤 Request 77/200 sent at 1.79s
✅ Request 13/200 completed
📤 Request 78/200 sent at 1.79s
📤 Request 79/200 sent at 1.80s
✅ Request 53/200 completed
✅ Request 46/200 completed
✅ Request 50/200 completed
✅ Request 44/200 completed
📤 Request 80/200 sent at 1.86s
📤 Request 81/200 sent at 1.87s
✅ Request 47/200 completed
✅ Request 7/200 completed
✅ Request 55/200 completed
✅ Request 20/200 completed
✅ Request 12/200 completed
✅ Request 23/200 completed
✅ Request 40/200 completed
📤 Request 82/200 sent at 1.93s
✅ Request 41/200 completed
✅ Request 48/200 completed
📤 Request 83/200 sent at 1.98s
✅ Request 43/200 completed
📤 Request 84/200 sent at 2.01s
📤 Request 85/200 sent at 2.03s
📤 Request 86/200 sent at 2.04s
✅ Request 63/200 completed
✅ Request 22/200 completed
📤 Request 87/200 sent at 2.07s
✅ Request 49/200 completed
📤 Request 88/200 sent at 2.08s
✅ Request 52/200 completed
✅ Request 15/200 completed
✅ Request 37/200 completed
📤 Request 89/200 sent at 2.10s
📤 Request 90/200 sent at 2.10s
✅ Request 28/200 completed
✅ Request 32/200 completed
📤 Request 91/200 sent at 2.13s
✅ Request 8/200 completed
✅ Request 56/200 completed
✅ Request 29/200 completed
✅ Request 4/200 completed
📤 Request 92/200 sent at 2.19s
📤 Request 93/200 sent at 2.22s
✅ Request 39/200 completed
📤 Request 94/200 sent at 2.24s
📤 Request 95/200 sent at 2.25s
✅ Request 34/200 completed
✅ Request 74/200 completed
✅ Request 25/200 completed
✅ Request 11/200 completed
✅ Request 33/200 completed
✅ Request 31/200 completed
📤 Request 96/200 sent at 2.38s
📤 Request 97/200 sent at 2.39s
✅ Request 38/200 completed
✅ Request 62/200 completed
✅ Request 59/200 completed
✅ Request 58/200 completed
✅ Request 75/200 completed
📤 Request 98/200 sent at 2.46s
✅ Request 67/200 completed
📤 Request 99/200 sent at 2.47s
✅ Request 66/200 completed
📤 Request 100/200 sent at 2.47s
✅ Request 36/200 completed
📤 Request 101/200 sent at 2.50s
📤 Request 102/200 sent at 2.51s
📤 Request 103/200 sent at 2.51s
📤 Request 104/200 sent at 2.54s
📤 Request 105/200 sent at 2.59s
📤 Request 106/200 sent at 2.60s
📤 Request 107/200 sent at 2.60s
📤 Request 108/200 sent at 2.60s
✅ Request 78/200 completed
✅ Request 79/200 completed
📤 Request 109/200 sent at 2.65s
✅ Request 60/200 completed
✅ Request 68/200 completed
✅ Request 51/200 completed
✅ Request 54/200 completed
📤 Request 110/200 sent at 2.72s
📤 Request 111/200 sent at 2.73s
📤 Request 112/200 sent at 2.73s
📤 Request 113/200 sent at 2.77s
📤 Request 114/200 sent at 2.79s
✅ Request 57/200 completed
✅ Request 64/200 completed
✅ Request 69/200 completed
✅ Request 87/200 completed
📤 Request 115/200 sent at 2.85s
📤 Request 116/200 sent at 2.86s
✅ Request 84/200 completed
📤 Request 117/200 sent at 2.91s
📤 Request 118/200 sent at 2.91s
📤 Request 119/200 sent at 2.92s
📤 Request 120/200 sent at 2.93s
✅ Request 70/200 completed
📤 Request 121/200 sent at 2.93s
📤 Request 122/200 sent at 2.98s
📤 Request 123/200 sent at 2.98s
📤 Request 124/200 sent at 3.00s
📤 Request 125/200 sent at 3.01s
📤 Request 126/200 sent at 3.02s
✅ Request 92/200 completed
✅ Request 65/200 completed
📤 Request 127/200 sent at 3.06s
📤 Request 128/200 sent at 3.07s
📤 Request 129/200 sent at 3.07s
📤 Request 130/200 sent at 3.07s
📤 Request 131/200 sent at 3.08s
📤 Request 132/200 sent at 3.09s
✅ Request 72/200 completed
📤 Request 133/200 sent at 3.11s
✅ Request 81/200 completed
✅ Request 61/200 completed
📤 Request 134/200 sent at 3.14s
✅ Request 89/200 completed
📤 Request 135/200 sent at 3.16s
✅ Request 71/200 completed
📤 Request 136/200 sent at 3.24s
✅ Request 100/200 completed
✅ Request 91/200 completed
📤 Request 137/200 sent at 3.27s
📤 Request 138/200 sent at 3.29s
📤 Request 139/200 sent at 3.29s
📤 Request 140/200 sent at 3.30s
📤 Request 141/200 sent at 3.30s
📤 Request 142/200 sent at 3.32s
📤 Request 143/200 sent at 3.34s
✅ Request 112/200 completed
📤 Request 144/200 sent at 3.34s
📤 Request 145/200 sent at 3.36s
✅ Request 80/200 completed
✅ Request 94/200 completed
✅ Request 103/200 completed
📤 Request 146/200 sent at 3.39s
✅ Request 76/200 completed
📤 Request 147/200 sent at 3.42s
✅ Request 73/200 completed
✅ Request 83/200 completed
📤 Request 148/200 sent at 3.44s
✅ Request 114/200 completed
📤 Request 149/200 sent at 3.46s
📤 Request 150/200 sent at 3.46s
📤 Request 151/200 sent at 3.47s
📤 Request 152/200 sent at 3.47s
📤 Request 153/200 sent at 3.47s
📤 Request 154/200 sent at 3.47s
📤 Request 155/200 sent at 3.49s
📤 Request 156/200 sent at 3.50s
📤 Request 157/200 sent at 3.52s
📤 Request 158/200 sent at 3.53s
✅ Request 95/200 completed
✅ Request 88/200 completed
📤 Request 159/200 sent at 3.55s
📤 Request 160/200 sent at 3.56s
📤 Request 161/200 sent at 3.57s
📤 Request 162/200 sent at 3.58s
📤 Request 163/200 sent at 3.58s
✅ Request 98/200 completed
📤 Request 164/200 sent at 3.59s
📤 Request 165/200 sent at 3.60s
📤 Request 166/200 sent at 3.62s
📤 Request 167/200 sent at 3.63s
📤 Request 168/200 sent at 3.63s
✅ Request 77/200 completed
📤 Request 169/200 sent at 3.64s
📤 Request 170/200 sent at 3.68s
✅ Request 82/200 completed
✅ Request 99/200 completed
✅ Request 110/200 completed
✅ Request 85/200 completed
📤 Request 171/200 sent at 3.75s
📤 Request 172/200 sent at 3.76s
📤 Request 173/200 sent at 3.78s
📤 Request 174/200 sent at 3.78s
✅ Request 144/200 completed
📤 Request 175/200 sent at 3.80s
📤 Request 176/200 sent at 3.80s
✅ Request 111/200 completed
📤 Request 177/200 sent at 3.82s
📤 Request 178/200 sent at 3.83s
📤 Request 179/200 sent at 3.84s
📤 Request 180/200 sent at 3.84s
📤 Request 181/200 sent at 3.85s
📤 Request 182/200 sent at 3.85s
✅ Request 107/200 completed
✅ Request 104/200 completed
📤 Request 183/200 sent at 3.89s
📤 Request 184/200 sent at 3.91s
📤 Request 185/200 sent at 3.93s
📤 Request 186/200 sent at 3.93s
📤 Request 187/200 sent at 3.94s
📤 Request 188/200 sent at 3.94s
📤 Request 189/200 sent at 3.95s
📤 Request 190/200 sent at 3.96s
📤 Request 191/200 sent at 3.98s
📤 Request 192/200 sent at 3.99s
📤 Request 193/200 sent at 3.99s
✅ Request 86/200 completed
📤 Request 194/200 sent at 4.00s
✅ Request 131/200 completed
📤 Request 195/200 sent at 4.02s
✅ Request 122/200 completed
✅ Request 90/200 completed
📤 Request 196/200 sent at 4.08s
📤 Request 197/200 sent at 4.10s
📤 Request 198/200 sent at 4.11s
📤 Request 199/200 sent at 4.13s
📤 Request 200/200 sent at 4.14s

Waiting for all requests to complete...
✅ Request 101/200 completed
✅ Request 97/200 completed
✅ Request 119/200 completed
✅ Request 147/200 completed
✅ Request 109/200 completed
✅ Request 116/200 completed
✅ Request 93/200 completed
✅ Request 120/200 completed
✅ Request 169/200 completed
✅ Request 163/200 completed
✅ Request 126/200 completed
✅ Request 115/200 completed
✅ Request 134/200 completed
✅ Request 105/200 completed
✅ Request 106/200 completed
✅ Request 121/200 completed
✅ Request 170/200 completed
✅ Request 102/200 completed
✅ Request 133/200 completed
✅ Request 175/200 completed
✅ Request 128/200 completed
✅ Request 155/200 completed
✅ Request 164/200 completed
✅ Request 96/200 completed
✅ Request 162/200 completed
✅ Request 186/200 completed
✅ Request 123/200 completed
✅ Request 142/200 completed
✅ Request 154/200 completed
✅ Request 118/200 completed
✅ Request 140/200 completed
✅ Request 171/200 completed
✅ Request 129/200 completed
✅ Request 151/200 completed
✅ Request 183/200 completed
✅ Request 108/200 completed
✅ Request 168/200 completed
✅ Request 132/200 completed
✅ Request 177/200 completed
✅ Request 143/200 completed
✅ Request 113/200 completed
✅ Request 181/200 completed
✅ Request 130/200 completed
✅ Request 153/200 completed
✅ Request 117/200 completed
✅ Request 148/200 completed
✅ Request 124/200 completed
✅ Request 135/200 completed
✅ Request 157/200 completed
✅ Request 192/200 completed
✅ Request 185/200 completed
✅ Request 156/200 completed
✅ Request 161/200 completed
✅ Request 196/200 completed
✅ Request 187/200 completed
✅ Request 127/200 completed
✅ Request 149/200 completed
✅ Request 199/200 completed
✅ Request 159/200 completed
✅ Request 125/200 completed
✅ Request 190/200 completed
✅ Request 191/200 completed
✅ Request 152/200 completed
✅ Request 174/200 completed
✅ Request 145/200 completed
✅ Request 139/200 completed
✅ Request 160/200 completed
✅ Request 150/200 completed
✅ Request 188/200 completed
✅ Request 137/200 completed
✅ Request 158/200 completed
✅ Request 165/200 completed
✅ Request 197/200 completed
✅ Request 195/200 completed
✅ Request 136/200 completed
✅ Request 146/200 completed
✅ Request 138/200 completed
✅ Request 172/200 completed
✅ Request 141/200 completed
✅ Request 166/200 completed
✅ Request 194/200 completed
✅ Request 179/200 completed
✅ Request 173/200 completed
✅ Request 182/200 completed
✅ Request 167/200 completed
✅ Request 180/200 completed
✅ Request 184/200 completed
✅ Request 178/200 completed
✅ Request 193/200 completed
✅ Request 176/200 completed
✅ Request 189/200 completed
✅ Request 198/200 completed
✅ Request 200/200 completed

✅ All requests sent in 5.45s

Querying request tracking information...
✅ Found tracking info for 200 requests

✅ Results exported to: router_test_20250729_211857.csv

📊 Summary Statistics:
Total requests: 200
Successful requests: 200
Failed requests: 0

Latency Statistics (successful requests):
  Server latency: mean=1.284s, p50=1.273s, p99=2.083s
  Total latency: mean=1.284s, p50=1.273s, p99=2.083s
  Queue time: mean=0.000s, p50=0.000s, p99=0.000s
  Server queue time (total): mean=0.568s, p50=0.518s, p99=1.016s

Host Distribution:
  http://localhost:60006: 100 requests (50.0%)
  http://localhost:60005: 100 requests (50.0%)

==========================================
测试完成！
结果已保存到: router_test_*.csv
==========================================


(lg) root:/nas/ganluo# python -m sglang.launch_server --model-path "/nas/models/Meta-Llama-3-8B-Instruct" --host "0.0.0.0" --port 60005 --base-gpu-id 2 --enable-metrics
[2025-07-29 21:12:05] server_args=ServerArgs(model_path='/nas/models/Meta-Llama-3-8B-Instruct', tokenizer_path='/nas/models/Meta-Llama-3-8B-Instruct', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=60005, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', mem_fraction_static=0.874, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, device='cuda', tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=859698659, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=2, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, crash_dump_folder=None, show_time_cost=False, enable_metrics=True, enable_metrics_for_all_schedulers=False, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, api_key=None, served_model_name='/nas/models/Meta-Llama-3-8B-Instruct', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, ep_size=1, enable_ep_moe=False, enable_deepep_moe=False, enable_flashinfer_moe=False, enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', hicache_io_backend='', hicache_storage_backend=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, cuda_graph_max_bs=None, cuda_graph_bs=None, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_nccl_nvls=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, enable_return_hidden_states=False, enable_triton_kernel_moe=False, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, num_reserved_decode_tokens=512, pdlb_url=None, custom_weight_loader=[], weight_loader_disable_mmap=False, enable_pdmux=False, sm_group_num=3)
[2025-07-29 21:12:12] Scheduler initialized with enable_metrics=True
[2025-07-29 21:12:12] Attention backend not explicitly specified. Use fa3 backend by default.
[2025-07-29 21:12:12] Init torch distributed begin.
[2025-07-29 21:12:13] Init torch distributed ends. mem usage=0.00 GB
[2025-07-29 21:12:13] Load weight begin. avail mem=78.49 GB
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.12it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.07it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.08it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.50it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.32it/s]

[2025-07-29 21:12:18] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=63.52 GB, mem usage=14.98 GB.
[2025-07-29 21:12:19] KV Cache is allocated. #tokens: 439212, K size: 26.81 GB, V size: 26.81 GB
[2025-07-29 21:12:19] Memory pool end. avail mem=9.70 GB
[2025-07-29 21:12:20] Capture cuda graph begin. This can take up to several minutes. avail mem=9.60 GB
[2025-07-29 21:12:22] Capture cuda graph bs [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160]
Capturing batches (bs=1 avail_mem=8.53 GB): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:10<00:00,  2.27it/s]
[2025-07-29 21:12:32] Capture cuda graph end. Time elapsed: 11.67 s. mem usage=1.07 GB. avail mem=8.53 GB.
[2025-07-29 21:12:32] max_total_num_tokens=439212, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=8192, available_gpu_mem=8.53 GB
[2025-07-29 21:12:33] INFO:     Started server process [41843]
[2025-07-29 21:12:33] INFO:     Waiting for application startup.
[2025-07-29 21:12:33] INFO:     Application startup complete.
[2025-07-29 21:12:33] INFO:     Uvicorn running on http://0.0.0.0:60005 (Press CTRL+C to quit)
[2025-07-29 21:12:34] INFO:     127.0.0.1:32814 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-07-29 21:12:34] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-07-29 21:12:35] INFO:     127.0.0.1:32820 - "POST /generate HTTP/1.1" 200 OK
[2025-07-29 21:12:35] The server is fired up and ready to roll!
[2025-07-29 21:16:27] INFO:     127.0.0.1:56370 - "GET /health HTTP/1.1" 200 OK
[2025-07-29 21:16:27] INFO:     127.0.0.1:56384 - "GET /health HTTP/1.1" 200 OK
[2025-07-29 21:16:37] INFO:     127.0.0.1:56646 - "GET /health HTTP/1.1" 200 OK


(lg) root:/nas/ganluo# python -m sglang.launch_server --model-path "/nas/models/Meta-Llama-3-8B-Instruct" --host "0.0.0.0" --port 60006 --base-gpu-id 3 --enable-metrics
[2025-07-29 21:12:13] server_args=ServerArgs(model_path='/nas/models/Meta-Llama-3-8B-Instruct', tokenizer_path='/nas/models/Meta-Llama-3-8B-Instruct', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=60006, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', mem_fraction_static=0.874, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, device='cuda', tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=278511057, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=3, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, crash_dump_folder=None, show_time_cost=False, enable_metrics=True, enable_metrics_for_all_schedulers=False, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, api_key=None, served_model_name='/nas/models/Meta-Llama-3-8B-Instruct', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, dp_size=1, load_balance_method='round_robin', dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, ep_size=1, enable_ep_moe=False, enable_deepep_moe=False, enable_flashinfer_moe=False, enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', hicache_io_backend='', hicache_storage_backend=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, cuda_graph_max_bs=None, cuda_graph_bs=None, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_nccl_nvls=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, enable_return_hidden_states=False, enable_triton_kernel_moe=False, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, num_reserved_decode_tokens=512, pdlb_url=None, custom_weight_loader=[], weight_loader_disable_mmap=False, enable_pdmux=False, sm_group_num=3)
[2025-07-29 21:12:22] Scheduler initialized with enable_metrics=True
[2025-07-29 21:12:22] Attention backend not explicitly specified. Use fa3 backend by default.
[2025-07-29 21:12:22] Init torch distributed begin.
[2025-07-29 21:12:25] Init torch distributed ends. mem usage=0.00 GB
[2025-07-29 21:12:26] Load weight begin. avail mem=78.49 GB
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.21it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.17it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.14it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.53it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.37it/s]

[2025-07-29 21:12:32] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=63.50 GB, mem usage=14.99 GB.
[2025-07-29 21:12:32] KV Cache is allocated. #tokens: 439065, K size: 26.80 GB, V size: 26.80 GB
[2025-07-29 21:12:32] Memory pool end. avail mem=9.66 GB
[2025-07-29 21:12:32] Capture cuda graph begin. This can take up to several minutes. avail mem=9.60 GB
[2025-07-29 21:12:32] Capture cuda graph bs [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160]
Capturing batches (bs=1 avail_mem=8.53 GB): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:06<00:00,  3.48it/s]
[2025-07-29 21:12:39] Capture cuda graph end. Time elapsed: 6.82 s. mem usage=1.07 GB. avail mem=8.53 GB.
[2025-07-29 21:12:39] max_total_num_tokens=439065, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=8192, available_gpu_mem=8.53 GB
[2025-07-29 21:12:40] INFO:     Started server process [42324]
[2025-07-29 21:12:40] INFO:     Waiting for application startup.
[2025-07-29 21:12:40] INFO:     Application startup complete.
[2025-07-29 21:12:40] INFO:     Uvicorn running on http://0.0.0.0:60006 (Press CTRL+C to quit)
[2025-07-29 21:12:41] INFO:     127.0.0.1:52770 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-07-29 21:12:41] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-07-29 21:12:42] INFO:     127.0.0.1:52772 - "POST /generate HTTP/1.1" 200 OK
[2025-07-29 21:12:42] The server is fired up and ready to roll!
[2025-07-29 21:16:27] INFO:     127.0.0.1:48708 - "GET /health HTTP/1.1" 200 OK
[2025-07-29 21:16:27] INFO:     127.0.0.1:48714 - "GET /health HTTP/1.1" 200 OK
[2025-07-29 21:16:37] INFO:     127.0.0.1:58160 - "GET /health HTTP/1.1" 200 OK
[2025-07-29 21:16:47] INFO:     127.0.0.1:41056 - "GET /health HTTP/1.1" 200 OK


(lg) root:/nas/ganluo/sglang# bash /nas/ganluo/sglang/bash_start_router.sh
==========================================
SGLang Router 启动配置
==========================================
路由策略: random
Worker节点: http://localhost:60005 http://localhost:60006
路由器地址: 0.0.0.0:60009
请求追踪: true
日志级别: INFO
==========================================

执行命令:
python /nas/ganluo/sglang/start_router.py --policy random --host 0.0.0.0 --port 60009 --log-level INFO --workers http://localhost:60005 http://localhost:60006 --enable-tracking --max-trace-entries 100000 --trace-ttl 3600

Port-GPU mapping saved to: /tmp/sglang_port_gpu_mapping.json
Mapping:
  Port 60005 -> cuda:0
  Port 60006 -> cuda:1

Starting router with configuration:
  Policy: random
  Workers: ['http://localhost:60005', 'http://localhost:60006']
  Host: 0.0.0.0:60009
  Request tracking: Enabled
  Max trace entries: 100000
  Trace TTL: 3600s
  Log level: INFO

Router started successfully! Press Ctrl+C to stop.
2025-07-29 13:16:27  INFO sglang_router_rs::server: src/server.rs:331: 🚧 Initializing Prometheus metrics on 127.0.0.1:29000
2025-07-29 13:16:27  INFO sglang_router_rs::server: src/server.rs:340: 🚧 Initializing router on 0.0.0.0:60009
2025-07-29 13:16:27  INFO sglang_router_rs::server: src/server.rs:341: 🚧 Router mode: Regular { worker_urls: ["http://localhost:60005", "http://localhost:60006"] }
2025-07-29 13:16:27  INFO sglang_router_rs::server: src/server.rs:342: 🚧 Policy: Random
2025-07-29 13:16:27  INFO sglang_router_rs::server: src/server.rs:343: 🚧 Max payload size: 256 MB
2025-07-29 13:16:27  INFO sglang_router_rs::server: src/server.rs:353: 🚧 Service discovery disabled
2025-07-29 13:16:27  INFO sglang_router_rs::routers::router: src/routers/router.rs:181: All workers are healthy
2025-07-29 13:16:27  INFO sglang_router_rs::request_tracker: src/request_tracker.rs:65: Initializing request tracker with max_entries=100000, ttl=3600s
2025-07-29 13:16:27  INFO sglang_router_rs::server: src/server.rs:390: ✅ Serving router on 0.0.0.0:60009
2025-07-29 13:16:27  INFO sglang_router_rs::server: src/server.rs:391: ✅ Serving workers on ["http://localhost:60005", "http://localhost:60006"]


(lg) root:/nas/ganluo/sglang# python verify_queue_fix.py 2>&1 | tee verify_queue_fix_newlog_60005.log

🚀 开始测试Queue时间戳修复
时间: 2025-07-29 21:21:10.730637
服务器: http://localhost:60005

============================================================
测试单个请求的Queue时间戳
============================================================

📋 响应内容:
{
  "error": {
    "message": "Either text, input_ids or input_embeds should be provided."
  }
}
❌ 响应中没有meta_info

============================================================
测试10个并发请求的Queue时间戳
============================================================

✅ 所有请求在 0.01秒 内完成

  请求 0: ❌ 没有meta_info
  请求 1: ❌ 没有meta_info
  请求 2: ❌ 没有meta_info
  请求 3: ❌ 没有meta_info
  请求 4: ❌ 没有meta_info
  请求 5: ❌ 没有meta_info
  请求 6: ❌ 没有meta_info
  请求 7: ❌ 没有meta_info
  请求 8: ❌ 没有meta_info
  请求 9: ❌ 没有meta_info

============================================================
测试总结
============================================================

❌ 测试失败！Queue时间戳仍然有问题

排查建议:
1. 确认代码已正确部署到服务器
2. 检查服务器启动参数是否包含--enable-metrics
3. 查看服务器日志中的enable_metrics状态
4. 使用--log-level debug查看详细的时间戳设置日志
(lg) root:/nas/ganluo/sglang# 


(lg) root:/nas/ganluo/sglang# python verify_queue_fix.py 2>&1 | tee verify_queue_fix_newlog_60006.log

🚀 开始测试Queue时间戳修复
时间: 2025-07-29 21:21:28.232818
服务器: http://localhost:60006

============================================================
测试单个请求的Queue时间戳
============================================================

📋 响应内容:
{
  "error": {
    "message": "Either text, input_ids or input_embeds should be provided."
  }
}
❌ 响应中没有meta_info

============================================================
测试10个并发请求的Queue时间戳
============================================================

✅ 所有请求在 0.01秒 内完成

  请求 0: ❌ 没有meta_info
  请求 1: ❌ 没有meta_info
  请求 2: ❌ 没有meta_info
  请求 3: ❌ 没有meta_info
  请求 4: ❌ 没有meta_info
  请求 5: ❌ 没有meta_info
  请求 6: ❌ 没有meta_info
  请求 7: ❌ 没有meta_info
  请求 8: ❌ 没有meta_info
  请求 9: ❌ 没有meta_info

============================================================
测试总结
============================================================

❌ 测试失败！Queue时间戳仍然有问题

排查建议:
1. 确认代码已正确部署到服务器
2. 检查服务器启动参数是否包含--enable-metrics
3. 查看服务器日志中的enable_metrics状态
4. 使用--log-level debug查看详细的时间戳设置日志
(lg) root:/nas/ganluo/sglang# 
